<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers</description>
<lastBuildDate>Sat, 21 Feb 2026 08:43:29 +0000</lastBuildDate>
<item>
<title>AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games</title>
<link>https://tldr.takara.ai/p/2602.17594</link>
<guid>title:ai gamestore scalable open ended evaluation of machine general intelligence with human games</guid>
<pubDate>Thu, 19 Feb 2026 18:17:25 +0000</pubDate>
<description>Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a &quot;human game&quot; to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the &quot;Multiverse of Human Games&quot;. Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>vision</category>
<category>world-models</category>
</item>
<item>
<title>OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</title>
<link>https://tldr.takara.ai/p/2602.17665</link>
<guid>title:openearthagent a unified framework for tool augmented geospatial agents</guid>
<pubDate>Thu, 19 Feb 2026 18:59:54 +0000</pubDate>
<description>Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>Sink-Aware Pruning for Diffusion Language Models</title>
<link>https://tldr.takara.ai/p/2602.17664</link>
<guid>title:sink aware pruning for diffusion language models</guid>
<pubDate>Thu, 19 Feb 2026 18:59:50 +0000</pubDate>
<description>Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>FAMOSE: A ReAct Approach to Automated Feature Discovery</title>
<link>https://tldr.takara.ai/p/2602.17641</link>
<guid>title:famose a react approach to automated feature discovery</guid>
<pubDate>Thu, 19 Feb 2026 18:53:15 +0000</pubDate>
<description>Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0. 23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2. 0% on average, while remaining more robust to errors than other algorithms.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Unmasking the Factual-Conceptual Gap in Persian Language Models</title>
<link>https://tldr.takara.ai/p/2602.17623</link>
<guid>title:unmasking the factual conceptual gap in persian language models</guid>
<pubDate>Thu, 19 Feb 2026 18:42:46 +0000</pubDate>
<description>While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Towards Anytime-Valid Statistical Watermarking</title>
<link>https://tldr.takara.ai/p/2602.17608</link>
<guid>title:towards anytime valid statistical watermarking</guid>
<pubDate>Thu, 19 Feb 2026 18:32:26 +0000</pubDate>
<description>The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>A Theoretical Framework for Modular Learning of Robust Generative Models</title>
<link>https://tldr.takara.ai/p/2602.17554</link>
<guid>title:a theoretical framework for modular learning of robust generative models</guid>
<pubDate>Thu, 19 Feb 2026 17:09:13 +0000</pubDate>
<description>Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities</title>
<link>https://tldr.takara.ai/p/2602.17402</link>
<guid>title:a contrastive variational autoencoder for nsclc survival prediction with missing modalities</guid>
<pubDate>Thu, 19 Feb 2026 14:29:34 +0000</pubDate>
<description>Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction</title>
<link>https://tldr.takara.ai/p/2602.17102</link>
<guid>title:operationalization of machine learning with serverless architecture an industrial operationalization of machine learning with serverless architecture an industrial implementation for harmonized system code prediction</guid>
<pubDate>Thu, 19 Feb 2026 05:59:55 +0000</pubDate>
<description>This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>MARS: Margin-Aware Reward-Modeling with Self-Refinement</title>
<link>https://tldr.takara.ai/p/2602.17658</link>
<guid>title:mars margin aware reward modeling with self refinement</guid>
<pubDate>Thu, 19 Feb 2026 18:59:03 +0000</pubDate>
<description>Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</title>
<link>https://tldr.takara.ai/p/2602.17654</link>
<guid>title:mine and refine optimizing graded relevance in e commerce search retrieval</guid>
<pubDate>Thu, 19 Feb 2026 18:56:36 +0000</pubDate>
<description>We propose a two-stage &quot;Mine and Refine&quot; contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</title>
<link>https://tldr.takara.ai/p/2602.17605</link>
<guid>title:adapting actively on the fly relevance guided online meta learning with latent concepts for geospatial discovery</guid>
<pubDate>Thu, 19 Feb 2026 18:30:18 +0000</pubDate>
<description>In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e. g. , land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Probability-Invariant Random Walk Learning on Gyral Folding-Based Cortical Similarity Networks for Alzheimer's and Lewy Body Dementia Diagnosis</title>
<link>https://tldr.takara.ai/p/2602.17557</link>
<guid>title:probability invariant random walk learning on gyral folding based cortical similarity networks for alzheimer s and lewy body dementia diagnosis</guid>
<pubDate>Thu, 19 Feb 2026 17:11:59 +0000</pubDate>
<description>Alzheimer's disease (AD) and Lewy body dementia (LBD) present overlapping clinical features yet require distinct diagnostic strategies. While neuroimaging-based brain network analysis is promising, atlas-based representations may obscure individualized anatomy. Gyral folding-based networks using three-hinge gyri provide a biologically grounded alternative, but inter-individual variability in cortical folding results in inconsistent landmark correspondence and highly irregular network sizes, violating the fixed-topology and node-alignment assumptions of most existing graph learning methods, particularly in clinical datasets where pathological changes further amplify anatomical heterogeneity. We therefore propose a probability-invariant random-walk-based framework that classifies individualized gyral folding networks without explicit node alignment. Cortical similarity networks are built from local morphometric features and represented by distributions of anonymized random walks, with an anatomy-aware encoding that preserves permutation invariance. Experiments on a large clinical cohort of AD and LBD subjects show consistent improvements over existing gyral folding and atlas-based models, demonstrating robustness and potential for dementia diagnosis.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs</title>
<link>https://tldr.takara.ai/p/2602.17535</link>
<guid>title:lata laplacian assisted transductive adaptation for conformal uncertainty in medical vlms</guid>
<pubDate>Thu, 19 Feb 2026 16:45:38 +0000</pubDate>
<description>Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data</title>
<link>https://tldr.takara.ai/p/2602.17483</link>
<guid>title:what do llms associate with your name a human centered black box audit of personal data</guid>
<pubDate>Thu, 19 Feb 2026 15:53:29 +0000</pubDate>
<description>Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e. g.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Flickering Multi-Armed Bandits</title>
<link>https://tldr.takara.ai/p/2602.17315</link>
<guid>title:flickering multi armed bandits</guid>
<pubDate>Thu, 19 Feb 2026 12:24:01 +0000</pubDate>
<description>We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i. i. d. Erdős--Rényi (ER) process and an Edge-Markovian process.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>robotics</category>
</item>
<item>
<title>Continual uncertainty learning</title>
<link>https://tldr.takara.ai/p/2602.17174</link>
<guid>title:continual uncertainty learning</guid>
<pubDate>Thu, 19 Feb 2026 08:39:42 +0000</pubDate>
<description>Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning</title>
<link>https://tldr.takara.ai/p/2602.17168</link>
<guid>title:badclip stealthy and persistent backdoors in multimodal contrastive learning</guid>
<pubDate>Thu, 19 Feb 2026 08:31:16 +0000</pubDate>
<description>Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</title>
<link>https://tldr.takara.ai/p/2602.17663</link>
<guid>title:clef hipe 2026 evaluating accurate and efficient person place relation extraction from multilingual historical texts</guid>
<pubDate>Thu, 19 Feb 2026 18:59:44 +0000</pubDate>
<description>HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (&quot;Has the person ever been at this place? &quot;) and $isAt$ (&quot;Is the person located at this place around publication time? &quot;) - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</title>
<link>https://tldr.takara.ai/p/2602.17632</link>
<guid>title:smac score matched actor critics for robust offline to online transfer</guid>
<pubDate>Thu, 19 Feb 2026 18:47:31 +0000</pubDate>
<description>Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses</title>
<link>https://tldr.takara.ai/p/2602.17084</link>
<guid>title:how ai coding agents communicate a study of pull request description characteristics and human review responses</guid>
<pubDate>Thu, 19 Feb 2026 05:06:31 +0000</pubDate>
<description>The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning</title>
<link>https://tldr.takara.ai/p/2602.17062</link>
<guid>title:retaining suboptimal actions to follow shifting optima in multi agent reinforcement learning</guid>
<pubDate>Thu, 19 Feb 2026 04:07:55 +0000</pubDate>
<description>Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Revisiting Weight Regularization for Low-Rank Continual Learning</title>
<link>https://tldr.takara.ai/p/2602.17559</link>
<guid>title:revisiting weight regularization for low rank continual learning</guid>
<pubDate>Thu, 19 Feb 2026 17:13:00 +0000</pubDate>
<description>Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Adaptive Decentralized Composite Optimization via Three-Operator Splitting</title>
<link>https://tldr.takara.ai/p/2602.17545</link>
<guid>title:adaptive decentralized composite optimization via three operator splitting</guid>
<pubDate>Thu, 19 Feb 2026 16:59:34 +0000</pubDate>
<description>The paper studies decentralized optimization over networks, where agents minimize a sum of {\it locally} smooth (strongly) convex losses and plus a nonsmooth convex extended value term. We propose decentralized methods wherein agents {\it adaptively} adjust their stepsize via local backtracking procedures coupled with lightweight min-consensus protocols. Our design stems from a three-operator splitting factorization applied to an equivalent reformulation of the problem. The reformulation is endowed with a new BCV preconditioning metric (Bertsekas-O'Connor-Vandenberghe), which enables efficient decentralized implementation and local stepsize adjustments. We establish robust convergence guarantees. Under mere convexity, the proposed methods converge with a sublinear rate.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Position: Evaluation of ECG Representations Must Be Fixed</title>
<link>https://tldr.takara.ai/p/2602.17531</link>
<guid>title:position evaluation of ecg representations must be fixed</guid>
<pubDate>Thu, 19 Feb 2026 16:42:46 +0000</pubDate>
<description>This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles</title>
<link>https://tldr.takara.ai/p/2602.17028</link>
<guid>title:forecasting anomaly precursors via uncertainty aware time series ensembles</guid>
<pubDate>Thu, 19 Feb 2026 02:48:09 +0000</pubDate>
<description>Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics</title>
<link>https://tldr.takara.ai/p/2602.17513</link>
<guid>title:bridging the domain divide supervised vs zero shot clinical section segmentation from mimic iii to obstetrics</guid>
<pubDate>Thu, 19 Feb 2026 16:25:07 +0000</pubDate>
<description>Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Learning with Boolean threshold functions</title>
<link>https://tldr.takara.ai/p/2602.17493</link>
<guid>title:learning with boolean threshold functions</guid>
<pubDate>Thu, 19 Feb 2026 16:07:25 +0000</pubDate>
<description>We develop a method for training neural networks on Boolean data in which the values at all nodes are strictly $\pm 1$, and the resulting models are typically equivalent to networks whose nonzero weights are also $\pm 1$. The method replaces loss minimization with a nonconvex constraint formulation. Each node implements a Boolean threshold function (BTF), and training is expressed through a divide-and-concur decomposition into two complementary constraints: one enforces local BTF consistency between inputs, weights, and output; the other imposes architectural concurrence, equating neuron outputs with downstream inputs and enforcing weight equality across training-data instantiations of the network. The reflect-reflect-relax (RRR) projection algorithm is used to reconcile these constraints. Each BTF constraint includes a lower bound on the margin. When this bound is sufficiently large, the learned representations are provably sparse and equivalent to networks composed of simple logical gates with $\pm 1$ weights.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Entropy-Based Data Selection for Language Models</title>
<link>https://tldr.takara.ai/p/2602.17465</link>
<guid>title:entropy based data selection for language models</guid>
<pubDate>Thu, 19 Feb 2026 15:29:34 +0000</pubDate>
<description>Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>ABCD: All Biases Come Disguised</title>
<link>https://tldr.takara.ai/p/2602.17445</link>
<guid>title:abcd all biases come disguised</guid>
<pubDate>Thu, 19 Feb 2026 15:12:33 +0000</pubDate>
<description>Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
</channel>
</rss>