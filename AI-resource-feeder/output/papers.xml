<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers</description>
<lastBuildDate>Sun, 08 Feb 2026 08:44:43 +0000</lastBuildDate>
<item>
<title>Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</title>
<link>https://tldr.takara.ai/p/2602.05827</link>
<guid>title:sparse video generation propels real world beyond the view vision language navigation</guid>
<pubDate>Thu, 05 Feb 2026 16:16:13 +0000</pubDate>
<description>Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>RISE-Video: Can Video Generators Decode Implicit World Rules?</title>
<link>https://tldr.takara.ai/p/2602.05986</link>
<guid>title:rise video can video generators decode implicit world rules</guid>
<pubDate>Thu, 05 Feb 2026 18:36:10 +0000</pubDate>
<description>While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>DFlash: Block Diffusion for Flash Speculative Decoding</title>
<link>https://tldr.takara.ai/p/2602.06036</link>
<guid>title:dflash block diffusion for flash speculative decoding</guid>
<pubDate>Thu, 05 Feb 2026 18:59:30 +0000</pubDate>
<description>Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</title>
<link>https://tldr.takara.ai/p/2602.06035</link>
<guid>title:interprior scaling generative control for physics based human object interactions</guid>
<pubDate>Thu, 05 Feb 2026 18:59:27 +0000</pubDate>
<description>Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>
<link>https://tldr.takara.ai/p/2602.05951</link>
<guid>title:better source better flow learning condition dependent source distribution for flow matching</guid>
<pubDate>Thu, 05 Feb 2026 18:08:20 +0000</pubDate>
<description>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Regularized Calibration with Successive Rounding for Post-Training Quantization</title>
<link>https://tldr.takara.ai/p/2602.05902</link>
<guid>title:regularized calibration with successive rounding for post training quantization</guid>
<pubDate>Thu, 05 Feb 2026 17:18:02 +0000</pubDate>
<description>Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>efficiency</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
<link>https://tldr.takara.ai/p/2602.05859</link>
<guid>title:dlm scope mechanistic interpretability of diffusion language models via sparse autoencoders</guid>
<pubDate>Thu, 05 Feb 2026 16:41:25 +0000</pubDate>
<description>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</title>
<link>https://tldr.takara.ai/p/2602.05633</link>
<guid>title:castle a comprehensive benchmark for evaluating student tailored personalized safety in large language models</guid>
<pubDate>Thu, 05 Feb 2026 13:13:19 +0000</pubDate>
<description>Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Multi-Task GRPO: Reliable LLM Reasoning Across Tasks</title>
<link>https://tldr.takara.ai/p/2602.05547</link>
<guid>title:multi task grpo reliable llm reasoning across tasks</guid>
<pubDate>Thu, 05 Feb 2026 11:06:37 +0000</pubDate>
<description>RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation</title>
<link>https://tldr.takara.ai/p/2602.05544</link>
<guid>title:reasoning guided collaborative filtering with language models for explainable recommendation</guid>
<pubDate>Thu, 05 Feb 2026 11:05:09 +0000</pubDate>
<description>Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7. 38\% in Sports and 4.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing</title>
<link>https://tldr.takara.ai/p/2602.05480</link>
<guid>title:soma 1m a large scale sar optical multi resolution alignment dataset for multi task remote sensing</guid>
<pubDate>Thu, 05 Feb 2026 09:39:49 +0000</pubDate>
<description>Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1. 3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0. 5 m to 10 m.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models</title>
<link>https://tldr.takara.ai/p/2602.05385</link>
<guid>title:iesr efficient mcts based modular reasoning for text to sql with large language models</guid>
<pubDate>Thu, 05 Feb 2026 07:10:45 +0000</pubDate>
<description>Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24. 28 EX) and the Archer dataset (37. 28 EX) using only compact lightweight models without fine-tuning.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies</title>
<link>https://tldr.takara.ai/p/2602.06015</link>
<guid>title:a systematic evaluation of large language models for ptsd severity estimation the role of contextual knowledge and modeling strategies</guid>
<pubDate>Thu, 05 Feb 2026 18:53:17 +0000</pubDate>
<description>Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025</title>
<link>https://tldr.takara.ai/p/2602.05930</link>
<guid>title:compound deception in elite peer review a failure mode taxonomy of 100 fabricated citations at neurips 2025</guid>
<pubDate>Thu, 05 Feb 2026 17:43:35 +0000</pubDate>
<description>Large language models (LLMs) are increasingly used in academic writing workflows, yet they frequently hallucinate by generating citations to sources that do not exist. This study analyzes 100 AI-generated hallucinated citations that appeared in papers accepted by the 2025 Conference on Neural Information Processing Systems (NeurIPS), one of the world's most prestigious AI conferences. Despite review by 3-5 expert researchers per paper, these fabricated citations evaded detection, appearing in 53 published papers (approx. 1% of all accepted papers). We develop a five-category taxonomy that classifies hallucinations by their failure mode: Total Fabrication (66%), Partial Attribute Corruption (27%), Identifier Hijacking (4%), Placeholder Hallucination (2%), and Semantic Hallucination (1%). Our analysis reveals a critical finding: every hallucination (100%) exhibited compound failure modes.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy</title>
<link>https://tldr.takara.ai/p/2602.05877</link>
<guid>title:agent2agent threats in safety critical llm assistants a human centric taxonomy</guid>
<pubDate>Thu, 05 Feb 2026 16:53:41 +0000</pubDate>
<description>The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous &quot;separation of concerns&quot; standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented &quot;victim modeling&quot; and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</title>
<link>https://tldr.takara.ai/p/2602.05832</link>
<guid>title:ui mem self evolving experience memory for online reinforcement learning in mobile gui agents</guid>
<pubDate>Thu, 05 Feb 2026 16:21:43 +0000</pubDate>
<description>Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes</title>
<link>https://tldr.takara.ai/p/2602.05780</link>
<guid>title:automated customization of llms for enterprise code repositories using semantic scopes</guid>
<pubDate>Thu, 05 Feb 2026 15:38:54 +0000</pubDate>
<description>Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Cross-Domain Offline Policy Adaptation via Selective Transition Correction</title>
<link>https://tldr.takara.ai/p/2602.05776</link>
<guid>title:cross domain offline policy adaptation via selective transition correction</guid>
<pubDate>Thu, 05 Feb 2026 15:37:29 +0000</pubDate>
<description>It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification</title>
<link>https://tldr.takara.ai/p/2602.05723</link>
<guid>title:mitigating hallucination in financial retrieval augmented generation via fine grained knowledge verification</guid>
<pubDate>Thu, 05 Feb 2026 14:49:05 +0000</pubDate>
<description>In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>A Mixed Reality System for Robust Manikin Localization in Childbirth Training</title>
<link>https://tldr.takara.ai/p/2602.05588</link>
<guid>title:a mixed reality system for robust manikin localization in childbirth training</guid>
<pubDate>Thu, 05 Feb 2026 12:17:05 +0000</pubDate>
<description>Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>vision</category>
</item>
<item>
<title>Ontology-Driven Robotic Specification Synthesis</title>
<link>https://tldr.takara.ai/p/2602.05456</link>
<guid>title:ontology driven robotic specification synthesis</guid>
<pubDate>Thu, 05 Feb 2026 08:59:23 +0000</pubDate>
<description>This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs</title>
<link>https://tldr.takara.ai/p/2602.05448</link>
<guid>title:blitzrank principled zero shot ranking agents with tournament graphs</guid>
<pubDate>Thu, 05 Feb 2026 08:41:00 +0000</pubDate>
<description>Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Reduced-Order Surrogates for Forced Flexible Mesh Coastal-Ocean Models</title>
<link>https://tldr.takara.ai/p/2602.05416</link>
<guid>title:reduced order surrogates for forced flexible mesh coastal ocean models</guid>
<pubDate>Thu, 05 Feb 2026 07:59:58 +0000</pubDate>
<description>While POD-based surrogates are widely explored for hydrodynamic applications, the use of Koopman Autoencoders for real-world coastal-ocean modelling remains relatively limited. This paper introduces a flexible Koopman autoencoder formulation that incorporates meteorological forcings and boundary conditions, and systematically compares its performance against POD-based surrogates. The Koopman autoencoder employs a learned linear temporal operator in latent space, enabling eigenvalue regularization to promote temporal stability. This strategy is evaluated alongside temporal unrolling techniques for achieving stable and accurate long-term predictions. The models are assessed on three test cases spanning distinct dynamical regimes, with prediction horizons up to one year at 30-minute temporal resolution. Across all cases, the Koopman autoencoder with temporal unrolling yields the best overall accuracy compared to the POD-based surrogates, achieving relative root-mean-squared-errors of 0.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better</title>
<link>https://tldr.takara.ai/p/2602.05393</link>
<guid>title:late to early training let llms learn earlier so faster and better</guid>
<pubDate>Thu, 05 Feb 2026 07:19:34 +0000</pubDate>
<description>As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models? } In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i. e. late training phase) model.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks</title>
<link>https://tldr.takara.ai/p/2602.05374</link>
<guid>title:cross lingual empirical evaluation of large language models for arabic medical tasks</guid>
<pubDate>Thu, 05 Feb 2026 06:52:46 +0000</pubDate>
<description>In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science</title>
<link>https://tldr.takara.ai/p/2602.05289</link>
<guid>title:towards a science of collective ai llm based multi agent systems need a transition from blind trial and error to rigorous science</guid>
<pubDate>Thu, 05 Feb 2026 04:19:52 +0000</pubDate>
<description>Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Î“$) as the scientific standard to isolate intrinsic gains from increased budgets.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Mechanisms of AI Protein Folding in ESMFold</title>
<link>https://tldr.takara.ai/p/2602.06020</link>
<guid>title:mechanisms of ai protein folding in esmfold</guid>
<pubDate>Thu, 05 Feb 2026 18:54:54 +0000</pubDate>
<description>How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Causal Inference on Stopped Random Walks in Online Advertising</title>
<link>https://tldr.takara.ai/p/2602.05997</link>
<guid>title:causal inference on stopped random walks in online advertising</guid>
<pubDate>Thu, 05 Feb 2026 18:43:29 +0000</pubDate>
<description>We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e. g. , Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e. g. , auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins</title>
<link>https://tldr.takara.ai/p/2602.05983</link>
<guid>title:geographically aware transformer based traffic forecasting for urban motorway digital twins</guid>
<pubDate>Thu, 05 Feb 2026 18:33:03 +0000</pubDate>
<description>The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling</title>
<link>https://tldr.takara.ai/p/2602.05202</link>
<guid>title:gt svj generative transformer based self supervised video judge for efficient video reward modeling</guid>
<pubDate>Thu, 05 Feb 2026 01:54:01 +0000</pubDate>
<description>Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
</channel>
</rss>