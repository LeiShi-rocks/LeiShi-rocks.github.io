<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Trend Feed</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers/blogs ranked by recency + popularity + relevance</description>
<lastBuildDate>Wed, 18 Feb 2026 08:59:52 +0000</lastBuildDate>
<item>
<title>1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization</title>
<link>https://tldr.takara.ai/p/2602.15563</link>
<guid>title:1 bit wonder improving qat performance in the low bit regime through k means quantization</guid>
<pubDate>Tue, 17 Feb 2026 13:23:26 +0000</pubDate>
<description>Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>efficiency</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</title>
<link>https://tldr.takara.ai/p/2602.15827</link>
<guid>title:perceptive humanoid parkour chaining dynamic human skills via motion matching</guid>
<pubDate>Tue, 17 Feb 2026 18:59:11 +0000</pubDate>
<description>While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
<category>vision</category>
</item>
<item>
<title>This human study did not involve human subjects: Validating LLM simulations as behavioral evidence</title>
<link>https://tldr.takara.ai/p/2602.15785</link>
<guid>title:this human study did not involve human subjects validating llm simulations as behavioral evidence</guid>
<pubDate>Tue, 17 Feb 2026 18:18:38 +0000</pubDate>
<description>A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra</title>
<link>https://tldr.takara.ai/p/2602.15669</link>
<guid>title:persona dynamic and compositional inference time personality control via activation vector algebra</guid>
<pubDate>Tue, 17 Feb 2026 15:47:58 +0000</pubDate>
<description>Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9. 60, nearly matching the supervised fine-tuning upper bound of 9.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</title>
<link>https://tldr.takara.ai/p/2602.15620</link>
<guid>title:stapo stabilizing reinforcement learning for llms by silencing rare spurious tokens</guid>
<pubDate>Tue, 17 Feb 2026 14:46:48 +0000</pubDate>
<description>Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0. 01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>The geometry of online conversations and the causal antecedents of conflictual discourse</title>
<link>https://tldr.takara.ai/p/2602.15600</link>
<guid>title:the geometry of online conversations and the causal antecedents of conflictual discourse</guid>
<pubDate>Tue, 17 Feb 2026 14:12:03 +0000</pubDate>
<description>This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Anthropic and Infosys collaborate to build AI agents for telecommunications and other regulated industries</title>
<link>https://www.anthropic.com/news/anthropic-infosys</link>
<guid>title:anthropic and infosys collaborate to build ai agents for telecommunications and other regulated industries</guid>
<pubDate>Wed, 18 Feb 2026 08:59:40 +0000</pubDate>
<description>Anthropic and Infosys , a global leader in next-generation digital services and consulting founded and headquartered in Bengaluru, today announced a collaboration to develop and deliver enterprise AI solutions across telecommunications, financial services, manufacturing, and software development. The collaboration integrates Anthropic’s Claude models and Claude Code with Infosys Topaz , an AI-first set of services, solutions, and platforms using generative and agentic AI technologies, to help companies speed up software development and adopt AI with the governance and transparency that regulated industries require. India is the second-largest market for Claude.ai, home to a developer community doing some of the most technically intense AI work we see anywhere: nearly half of Claude usage in India involves building applications, modernizing systems, and shipping production software. Infosys is one of the first partners in Anthropic’s expanded presence in India .</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>LLM-as-Judge on a Budget</title>
<link>https://tldr.takara.ai/p/2602.15481</link>
<guid>title:llm as judge on a budget</guid>
<pubDate>Tue, 17 Feb 2026 10:35:41 +0000</pubDate>
<description>LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? % We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search</title>
<link>https://tldr.takara.ai/p/2602.15423</link>
<guid>title:gaiaflow semantic guided diffusion tuning for carbon frugal search</guid>
<pubDate>Tue, 17 Feb 2026 08:35:11 +0000</pubDate>
<description>As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety</title>
<link>https://tldr.takara.ai/p/2602.15799</link>
<guid>title:the geometry of alignment collapse when fine tuning breaks safety</guid>
<pubDate>Tue, 17 Feb 2026 18:39:15 +0000</pubDate>
<description>Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation</title>
<link>https://tldr.takara.ai/p/2602.15778</link>
<guid>title:pluie personalisable metric with llm used for improved evaluation</guid>
<pubDate>Tue, 17 Feb 2026 18:10:00 +0000</pubDate>
<description>Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution</title>
<link>https://tldr.takara.ai/p/2602.15769</link>
<guid>title:vitab a evaluating multimodal large language models on visual table attribution</guid>
<pubDate>Tue, 17 Feb 2026 18:01:35 +0000</pubDate>
<description>Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos</title>
<link>https://tldr.takara.ai/p/2602.15757</link>
<guid>title:beyond binary classification detecting fine grained sexism in social media videos</guid>
<pubDate>Tue, 17 Feb 2026 17:45:28 +0000</pubDate>
<description>Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models</title>
<link>https://tldr.takara.ai/p/2602.15684</link>
<guid>title:estimating human muscular fatigue in dynamic collaborative robotic tasks with learning based models</guid>
<pubDate>Tue, 17 Feb 2026 16:08:11 +0000</pubDate>
<description>Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry</title>
<link>https://tldr.takara.ai/p/2602.15676</link>
<guid>title:relative geometry of neural forecasters linking accuracy and alignment in learned latent geometry</guid>
<pubDate>Tue, 17 Feb 2026 16:00:08 +0000</pubDate>
<description>Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models</title>
<link>https://tldr.takara.ai/p/2602.15332</link>
<guid>title:directional reasoning trajectory change drtc identifying critical trace segments in reasoning models</guid>
<pubDate>Tue, 17 Feb 2026 03:38:16 +0000</pubDate>
<description>Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Visual Persuasion: What Influences Decisions of Vision-Language Models?</title>
<link>https://tldr.takara.ai/p/2602.15278</link>
<guid>title:visual persuasion what influences decisions of vision language models</guid>
<pubDate>Tue, 17 Feb 2026 00:33:53 +0000</pubDate>
<description>The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background).</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>Quantifying construct validity in large language model evaluations</title>
<link>https://tldr.takara.ai/p/2602.15532</link>
<guid>title:quantifying construct validity in large language model evaluations</guid>
<pubDate>Tue, 17 Feb 2026 12:15:57 +0000</pubDate>
<description>The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance. Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Anthropic opens Bengaluru office and announces new partnerships across India</title>
<link>https://www.anthropic.com/news/bengaluru-office-partnerships-across-india</link>
<guid>title:anthropic opens bengaluru office and announces new partnerships across india</guid>
<pubDate>Wed, 18 Feb 2026 08:59:41 +0000</pubDate>
<description>India is the second-largest market for Claude. ai , home to a developer community doing some of the most technically intense AI work we see anywhere. Nearly half of Claude usage in India comprises computer and mathematical tasks: building applications, modernizing systems, and shipping production software. Today, as we officially open our Bengaluru office, we’re announcing partnerships across enterprise, education, and agriculture that deepen our commitment to India across a range of sectors. “India represents one of the world’s most promising opportunities to bring the benefits of responsible AI to vastly more people and enterprises,” said Irina Ghose, Managing Director of India, Anthropic. “Already, it’s home to extraordinary technical talent, digital infrastructure at scale, and a proven track record of using technology to improve people’s lives.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Anthropic raises $30 billion in Series G funding at $380 billion post-money valuation</title>
<link>https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation</link>
<guid>title:anthropic raises 30 billion in series g funding at 380 billion post money valuation</guid>
<pubDate>Wed, 18 Feb 2026 08:59:39 +0000</pubDate>
<description>We have raised $30 billion in Series G funding led by GIC and Coatue, valuing Anthropic at $380 billion post-money. The round was co-led by D. E. Shaw Ventures, Dragoneer, Founders Fund, ICONIQ, and MGX. The investment will fuel the frontier research, product development, and infrastructure expansions that have made Anthropic the market leader in enterprise AI and coding. Significant investors in this round include: Accel, Addition, Alpha Wave Global, Altimeter, AMP PBC, Appaloosa LP, Baillie Gifford, Bessemer Venture Partners, affiliated funds of BlackRock, Blackstone, D1 Capital Partners, Fidelity Management &amp; Research Company, General Catalyst, Greenoaks, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, JPMorganChase through its Security and Resiliency Initiative and Growth Equity Partners, Lightspeed Venture Partners, Menlo Ventures, Morgan Stanley Investment Management, NX1 Capital, Qatar Investment Authority (QIA), Sands Capital, Sequoia Capital, Temasek, TowerBrook, TPG, Whale Rock Capital, and XN.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Wed, 18 Feb 2026 08:59:38 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta 1 .</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Wed, 18 Feb 2026 08:59:37 +0000</pubDate>
<description>Sonnet 4. 6 delivers frontier performance across coding, agents, and professional work at scale. We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution</title>
<link>https://tldr.takara.ai/p/2602.15830</link>
<guid>title:ensemble size dependence of deep learning post processing methods that minimize an un fair score motivating examples and a proof of concept solution</guid>
<pubDate>Tue, 17 Feb 2026 18:59:55 +0000</pubDate>
<description>Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Neural Scaling Laws for Boosted Jet Tagging</title>
<link>https://tldr.takara.ai/p/2602.15781</link>
<guid>title:neural scaling laws for boosted jet tagging</guid>
<pubDate>Tue, 17 Feb 2026 18:13:01 +0000</pubDate>
<description>The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac</title>
<link>https://tldr.takara.ai/p/2602.15753</link>
<guid>title:under resourced studies of under resourced languages lemmatization and pos tagging with llm annotators for historical armenian georgian greek and syriac</guid>
<pubDate>Tue, 17 Feb 2026 17:34:32 +0000</pubDate>
<description>Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Automated Multi-Source Debugging and Natural Language Error Explanation for Dashboard Applications</title>
<link>https://tldr.takara.ai/p/2602.15362</link>
<guid>title:automated multi source debugging and natural language error explanation for dashboard applications</guid>
<pubDate>Tue, 17 Feb 2026 05:06:28 +0000</pubDate>
<description>Modern web dashboards and enterprise applications increasingly rely on complex, distributed microservices architectures. While these architectures offer scalability, they introduce significant challenges in debugging and observability. When failures occur, they often manifest as opaque error messages to the end-user such as Something went wrong. This masks the underlying root cause which may reside in browser side exceptions, API contract violations, or server side logic failures. Existing monitoring tools capture these events in isolation but fail to correlate them effectively or provide intelligible explanations to non technical users. This paper proposes a novel system for Automated Multi Source Debugging and Natural Language Error Explanation.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Random Wavelet Features for Graph Kernel Machines</title>
<link>https://tldr.takara.ai/p/2602.15711</link>
<guid>title:random wavelet features for graph kernel machines</guid>
<pubDate>Tue, 17 Feb 2026 16:45:15 +0000</pubDate>
<description>Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>How to Disclose? Strategic AI Disclosure in Crowdfunding</title>
<link>https://tldr.takara.ai/p/2602.15698</link>
<guid>title:how to disclose strategic ai disclosure in crowdfunding</guid>
<pubDate>Tue, 17 Feb 2026 16:26:03 +0000</pubDate>
<description>As artificial intelligence (AI) increasingly integrates into crowdfunding practices, strategic disclosure of AI involvement has become critical. Yet, empirical insights into how different disclosure strategies influence investor decisions remain limited. Drawing on signaling theory and Aristotle's rhetorical framework, we examine how mandatory AI disclosure affects crowdfunding performance and how substantive signals (degree of AI involvement) and rhetorical signals (logos/explicitness, ethos/authenticity, pathos/emotional tone) moderate these effects. Leveraging Kickstarter's mandatory AI disclosure policy as a natural experiment and four supplementary online experiments, we find that mandatory AI disclosure significantly reduces crowdfunding performance: funds raised decline by 39. 8% and backer counts by 23. 9% for AI-involved projects.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Fine-Tuning LLMs to Generate Economical and Reliable Actions for the Power Grid</title>
<link>https://tldr.takara.ai/p/2602.15350</link>
<guid>title:fine tuning llms to generate economical and reliable actions for the power grid</guid>
<pubDate>Tue, 17 Feb 2026 04:33:02 +0000</pubDate>
<description>Public Safety Power Shutoffs (PSPS) force rapid topology changes that can render standard operating points infeasible, requiring operators to quickly identify corrective transmission switching actions that reduce load shedding while maintaining acceptable voltage behavior. We present a verifiable, multi-stage adaptation pipeline that fine-tunes an instruction-tuned large language model (LLM) to generate \emph{open-only} corrective switching plans from compact PSPS scenario summaries under an explicit switching budget. First, supervised fine-tuning distills a DC-OPF MILP oracle into a constrained action grammar that enables reliable parsing and feasibility checks. Second, direct preference optimization refines the policy using AC-evaluated preference pairs ranked by a voltage-penalty metric, injecting voltage-awareness beyond DC imitation. Finally, best-of-$N$ selection provides an inference-time addition by choosing the best feasible candidate under the target metric. On IEEE 118-bus PSPS scenarios, fine-tuning substantially improves DC objective values versus zero-shot generation, reduces AC power-flow failure from 50\% to single digits, and improves voltage-penalty outcomes on the common-success set.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset</title>
<link>https://tldr.takara.ai/p/2602.15349</link>
<guid>title:cremd crowd sourced emotional multimodal dogs dataset</guid>
<pubDate>Tue, 17 Feb 2026 04:31:38 +0000</pubDate>
<description>Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e. g. , context, audio, video) and annotator characteristics (e. g.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models</title>
<link>https://tldr.takara.ai/p/2602.15344</link>
<guid>title:er mia black box adversarial memory injection attacks on long term memory augmented large language models</guid>
<pubDate>Tue, 17 Feb 2026 04:19:45 +0000</pubDate>
<description>Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>Continuous-Time Piecewise-Linear Recurrent Neural Networks</title>
<link>https://tldr.takara.ai/p/2602.15649</link>
<guid>title:continuous time piecewise linear recurrent neural networks</guid>
<pubDate>Tue, 17 Feb 2026 15:16:12 +0000</pubDate>
<description>In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models</title>
<link>https://tldr.takara.ai/p/2602.15315</link>
<guid>title:training free zero shot anomaly detection in 3d brain mri with 2d foundation models</guid>
<pubDate>Tue, 17 Feb 2026 02:46:45 +0000</pubDate>
<description>Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory</title>
<link>https://tldr.takara.ai/p/2602.15313</link>
<guid>title:mnemis dual route retrieval on hierarchical graphs for long term llm memory</guid>
<pubDate>Tue, 17 Feb 2026 02:44:03 +0000</pubDate>
<description>AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>Anthropic and the Government of Rwanda sign MOU for AI in health and education</title>
<link>https://www.anthropic.com/news/anthropic-rwanda-mou</link>
<guid>title:anthropic and the government of rwanda sign mou for ai in health and education</guid>
<pubDate>Wed, 18 Feb 2026 08:59:40 +0000</pubDate>
<description>The Government of Rwanda and Anthropic have signed a three-year Memorandum of Understanding to formalize and expand our partnership, bringing AI to Rwanda’s education, health, and public sector systems. This agreement builds on the ALX education partnership we announced in November 2025 and marks the first time Anthropic has formalized a multi-sector partnership through a government MOU on the African continent. Our collaboration spans three areas: “This partnership with Anthropic is an important milestone in Rwanda’s AI journey. Our goal is to continue to design and deploy AI solutions that can be applied at a national level to strengthen education, advance health outcomes, and enhance governance with an emphasis on our context,” said Paula Ingabire, Minister of Information and Communications Technology (ICT) and Innovation in Rwanda.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Wed, 18 Feb 2026 08:59:39 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Our 2026 Responsible AI Progress Report</title>
<link>https://blog.google/innovation-and-ai/products/responsible-ai-2026-report-ongoing-work/</link>
<guid>title:our 2026 responsible ai progress report</guid>
<pubDate>Tue, 17 Feb 2026 22:30:00 +0000</pubDate>
<description>an illustration of blue and white cubes</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Teaching AI to read a map</title>
<link>https://research.google/blog/teaching-ai-to-read-a-map/</link>
<guid>title:teaching ai to read a map</guid>
<pubDate>Tue, 17 Feb 2026 21:37:00 +0000</pubDate>
<description>Machine Perception</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>google-research</category>
<category>machine perception</category>
<category>models</category>
<category>nonpaper</category>
<category>open source models &amp; datasets</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>Build AI-Ready Knowledge Systems Using 5 Essential Multimodal RAG Capabilities</title>
<link>https://developer.nvidia.com/blog/build-ai-ready-knowledge-systems-using-5-essential-multimodal-rag-capabilities/</link>
<guid>title:build ai ready knowledge systems using 5 essential multimodal rag capabilities</guid>
<pubDate>Tue, 17 Feb 2026 18:00:00 +0000</pubDate>
<description>Enterprise data is inherently complex: real-world documents are multimodal, spanning text, tables, charts and graphs, images, diagrams, scanned pages, forms,...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai agent</category>
<category>data center / cloud</category>
<category>hardware</category>
<category>infra</category>
<category>llms</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>retrieval augmented generation (rag)</category>
<category>rl</category>
<category>training</category>
</item>
<item>
<title>Gemini 3 Deep Think: Advancing science, research and engineering</title>
<link>https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/</link>
<guid>title:gemini 3 deep think advancing science research and engineering</guid>
<pubDate>Thu, 12 Feb 2026 16:13:00 +0000</pubDate>
<description>Gemini 3 Deep Think logo</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini models</category>
<category>google</category>
<category>google deepmind</category>
<category>google one</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Scheduling in a changing world: Maximizing throughput with time-varying capacity</title>
<link>https://research.google/blog/scheduling-in-a-changing-world-maximizing-throughput-with-time-varying-capacity/</link>
<guid>title:scheduling in a changing world maximizing throughput with time varying capacity</guid>
<pubDate>Wed, 11 Feb 2026 10:34:00 +0000</pubDate>
<description>Algorithms &amp; Theory</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>algorithms &amp; theory</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>rl</category>
<category>science</category>
</item>
<item>
<title>R²D²: Scaling Multimodal Robot Learning with NVIDIA Isaac Lab</title>
<link>https://developer.nvidia.com/blog/r2d2-scaling-multimodal-robot-learning-with-nvidia-isaac-lab/</link>
<guid>title:r d scaling multimodal robot learning with nvidia isaac lab</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Building robust, intelligent robots requires testing them in complex environments. However, gathering data in the physical world is expensive, slow, and often...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>ai foundation models</category>
<category>hardware</category>
<category>humanoid robots</category>
<category>infra</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>nvidia research</category>
<category>open source</category>
<category>physical ai</category>
<category>rl</category>
<category>robotics</category>
<category>robotics research and development digest (r²d²)</category>
<category>simulation / modeling / design</category>
<category>training</category>
</item>
<item>
<title>Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations</title>
<link>https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/</link>
<guid>title:beyond one on one authoring simulating and testing dynamic human ai group conversations</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Human-Computer Interaction and Visualization</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>google-research</category>
<category>human-computer interaction and visualization</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>9 fun questions to try asking Google Photos</title>
<link>https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/</link>
<guid>title:9 fun questions to try asking google photos</guid>
<pubDate>Tue, 10 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of outdoor images, a blue icon that say &quot;Ask Photos,&quot; and examples of Ask Photos prompts.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Anthropic AI safety researcher quits, says the ‘world is in peril’</title>
<link>https://globalnews.ca/news/11664538/anthropic-ai-safety-researcher-mrinank-sharma-quits-concerns/</link>
<guid>title:anthropic ai safety researcher quits says the world is in peril</guid>
<pubDate>Thu, 12 Feb 2026 17:00:46 +0000</pubDate>
<description>Anthropic was founded in 2021 by a breakaway group of former OpenAI employees who pledged to design a more safety-centric approach to AI development.</description>
<source url="https://globalnews.ca/tag/artificial-intelligence/feed">globalnews.ca</source>
<category>ai</category>
<category>artificial intelligence</category>
<category>globalnews.ca</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>tech</category>
<category>trending</category>
<category>world</category>
</item>
<item>
<title>Helping kids and teens learn and grow online on Safer Internet Day</title>
<link>https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/</link>
<guid>title:helping kids and teens learn and grow online on safer internet day</guid>
<pubDate>Tue, 10 Feb 2026 02:30:00 +0000</pubDate>
<description>User profile on smartphone connected to security, media, and settings icons.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>safety &amp; security</category>
</item>
<item>
<title>Automating Inference Optimizations with NVIDIA TensorRT LLM AutoDeploy</title>
<link>https://developer.nvidia.com/blog/automating-inference-optimizations-with-nvidia-tensorrt-llm-autodeploy/</link>
<guid>title:automating inference optimizations with nvidia tensorrt llm autodeploy</guid>
<pubDate>Mon, 09 Feb 2026 18:30:00 +0000</pubDate>
<description>NVIDIA TensorRT LLM enables developers to build high-performance inference engines for large language models (LLMs), but deploying a new architecture...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai inference</category>
<category>developer tools &amp; techniques</category>
<category>hardware</category>
<category>inference performance</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>mlops</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>pytorch</category>
<category>serving</category>
<category>training</category>
</item>
<item>
<title>How AI trained on birds is surfacing underwater mysteries</title>
<link>https://research.google/blog/how-ai-trained-on-birds-is-surfacing-underwater-mysteries/</link>
<guid>title:how ai trained on birds is surfacing underwater mysteries</guid>
<pubDate>Mon, 09 Feb 2026 18:38:00 +0000</pubDate>
<description>Climate &amp; Sustainability</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>climate &amp; sustainability</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>open source models &amp; datasets</category>
<category>research</category>
<category>science</category>
<category>sound &amp; accoustics</category>
</item>
<item>
<title>Natively Adaptive Interfaces: A new framework for AI accessibility</title>
<link>https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/</link>
<guid>title:natively adaptive interfaces a new framework for ai accessibility</guid>
<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>accessibility</category>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Build with Kimi K2.5 Multimodal VLM Using NVIDIA GPU-Accelerated Endpoints</title>
<link>https://developer.nvidia.com/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/</link>
<guid>title:build with kimi k2 5 multimodal vlm using nvidia gpu accelerated endpoints</guid>
<pubDate>Wed, 04 Feb 2026 19:46:33 +0000</pubDate>
<description>Kimi K2.5 is the newest open vision language model (VLM) from the Kimi family of models. Kimi K2.5 is a general-purpose multimodal model that excels in current...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai agent</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>top stories</category>
<category>training</category>
<category>vision</category>
<category>vlms</category>
</item>
<item>
<title>How Google Cloud is helping Team USA elevate their tricks with AI</title>
<link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/</link>
<guid>title:how google cloud is helping team usa elevate their tricks with ai</guid>
<pubDate>Thu, 05 Feb 2026 16:00:00 +0000</pubDate>
<description>A woman outdoors in the snow looks at a tablet. A half pipe is behind her.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google cloud</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Watch our new Gemini ad ahead of football’s biggest weekend</title>
<link>https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/</link>
<guid>title:watch our new gemini ad ahead of football s biggest weekend</guid>
<pubDate>Thu, 05 Feb 2026 14:30:00 +0000</pubDate>
<description>A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial’</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>How AI tools can redefine universal design to increase accessibility</title>
<link>https://research.google/blog/how-ai-agents-can-redefine-universal-design-to-increase-accessibility/</link>
<guid>title:how ai tools can redefine universal design to increase accessibility</guid>
<pubDate>Thu, 05 Feb 2026 08:28:00 +0000</pubDate>
<description>Education Innovation</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>education innovation</category>
<category>google-research</category>
<category>machine intelligence</category>
<category>models</category>
<category>natural language processing</category>
<category>nonpaper</category>
<category>research</category>
<category>responsible ai</category>
<category>science</category>
</item>
<item>
<title>The latest AI news we announced in January</title>
<link>https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/</link>
<guid>title:the latest ai news we announced in january</guid>
<pubDate>Wed, 04 Feb 2026 16:55:00 +0000</pubDate>
<description>mp4 showing a carousel of images including a card reading &quot;Help that's made for you&quot;</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>arts &amp; culture</category>
<category>chrome</category>
<category>developer tools</category>
<category>gemini</category>
<category>gemini app</category>
<category>gmail</category>
<category>google</category>
<category>google ads</category>
<category>google cloud</category>
<category>google deepmind</category>
<category>lab</category>
<category>learning &amp; education</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>search</category>
<category>shopping</category>
</item>
<item>
<title>​Sequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
<link>https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/</link>
<guid>title:sequential attention making ai models leaner and faster without sacrificing accuracy</guid>
<pubDate>Wed, 04 Feb 2026 15:14:00 +0000</pubDate>
<description>Algorithms &amp; Theory</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>algorithms &amp; theory</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference—far beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>How to Build a Document Processing Pipeline for RAG with Nemotron</title>
<link>https://developer.nvidia.com/blog/how-to-build-a-document-processing-pipeline-for-rag-with-nemotron/</link>
<guid>title:how to build a document processing pipeline for rag with nemotron</guid>
<pubDate>Wed, 04 Feb 2026 16:00:00 +0000</pubDate>
<description>What if your AI agent could instantly parse complex PDFs, extract nested tables, and &quot;see&quot; data within charts as easily as reading a text file? With NVIDIA...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>agents</category>
<category>build ai agents</category>
<category>data science</category>
<category>developer tools &amp; techniques</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm techniques</category>
<category>llms</category>
<category>nemo</category>
<category>nemo retriever</category>
<category>nemotron</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>retrieval augmented generation (rag)</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>How to Build License-Compliant Synthetic Data Pipelines for AI Model Distillation</title>
<link>https://developer.nvidia.com/blog/how-to-build-license-compliant-synthetic-data-pipelines-for-ai-model-distillation/</link>
<guid>title:how to build license compliant synthetic data pipelines for ai model distillation</guid>
<pubDate>Thu, 05 Feb 2026 18:00:00 +0000</pubDate>
<description>Specialized AI models are built to perform specific tasks or solve particular problems. But if you’ve ever tried to fine-tune or distill a domain-specific...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llms</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>pandas</category>
<category>synthetic data generation</category>
<category>training</category>
<category>training ai models</category>
</item>
<item>
<title>How Painkiller RTX Uses Generative AI to Modernize Game Assets at Scale</title>
<link>https://developer.nvidia.com/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/</link>
<guid>title:how painkiller rtx uses generative ai to modernize game assets at scale</guid>
<pubDate>Thu, 05 Feb 2026 14:00:00 +0000</pubDate>
<description>Painkiller RTX sets a new standard for how small teams can balance massive visual ambition with limited resources by integrating generative AI. By upscaling...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>content creation / rendering</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>news</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling</title>
<link>https://arxiv.org/abs/2602.15513</link>
<guid>arxiv:2602.15513</guid>
<pubDate>Wed, 18 Feb 2026 05:00:00 +0000</pubDate>
<description>arXiv:2602. 15513v1 Announce Type: cross Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization.</description>
<source url="https://rss.arxiv.org/rss/cs.AI">arxiv</source>
<category>agents</category>
<category>ai</category>
<category>alignment</category>
<category>arxiv</category>
<category>cs.ai</category>
<category>cs.ro</category>
<category>llm</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Will reward-seekers respond to distant incentives?</title>
<link>https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives</link>
<guid>title:will reward seekers respond to distant incentives</guid>
<pubDate>Mon, 16 Feb 2026 19:35:12 +0000</pubDate>
<description>Published on February 16, 2026 7:35 PM GMT Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e. g. , by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives.</description>
<source url="https://www.alignmentforum.org/feed.xml">alignment-forum</source>
<category>agents</category>
<category>alignment</category>
<category>alignment-forum</category>
<category>governance</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>The Future of Information Retrieval: From Dense Vectors to Cognitive Search</title>
<link>https://podcasters.spotify.com/pod/show/mlops/episodes/The-Future-of-Information-Retrieval-From-Dense-Vectors-to-Cognitive-Search-e3f7el9</link>
<guid>title:the future of information retrieval from dense vectors to cognitive search</guid>
<pubDate>Tue, 17 Feb 2026 18:00:11 +0000</pubDate>
<description>Rahul Raja is a Staff Software Engineer at LinkedIn , working on large-scale search infrastructure, information retrieval systems, and integrating AI/ML to improve ranking and semantic search experiences. The Future of Information Retrieval: From Dense Vectors to Cognitive Search // MLOps Podcast #362 with Rahul Raja, Staff Software Engineer at LinkedInJoin the Community: https://go. mlops. community/YTJoinInGet the newsletter: https://go. mlops. community/YTNewsletterMLOps GPU Guide: https://go.</description>
<source url="https://anchor.fm/s/174cb1b8/podcast/rss">anchor.fm</source>
<category>agents</category>
<category>ai</category>
<category>anchor.fm</category>
<category>llm</category>
<category>multimodal</category>
<category>news</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>vision</category>
</item>
<item>
<title>Introducing WorldVQA â</title>
<link>https://www.kimi.com/blog/worldvqa.html</link>
<guid>title:introducing worldvqa</guid>
<pubDate>Wed, 18 Feb 2026 08:59:44 +0000</pubDate>
<description>A benchmark for evaluating atomic visual world knowledge in Multimodal LLMs. Authors Kimi Team We are releasing WorldVQA , a new benchmark designed to measure the factual correctness of Multimodal Large Language Models (MLLMs). While recent models have demonstrated impressive capabilities in visual reasoning and description, measuring their reliability regarding visual world knowledge remains a challenge.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>kimi</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>I built an open-source Vercel alternative in Rust — here's what I learned</title>
<link>https://dev.to/david_viejo_4d48fdfa7cfff/i-built-an-open-source-vercel-alternative-in-rust-heres-what-i-learned-3oel</link>
<guid>title:i built an open source vercel alternative in rust here s what i learned</guid>
<pubDate>Wed, 18 Feb 2026 08:54:40 +0000</pubDate>
<description>I come from a DevOps and blockchain background. I've spent years managing infrastructure, wrangling containers, and thinking about how systems should be architected. So when I started shipping web apps and saw what developers were paying for deployment platforms, something felt off. Vercel's DX is incredible — I won't pretend otherwise. git push and your app is live. But then you look at the bill: $20/seat/month.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>dev.to</category>
<category>devops</category>
<category>news</category>
<category>nonpaper</category>
<category>opensource</category>
<category>rl</category>
<category>rust</category>
<category>serving</category>
<category>vision</category>
<category>webdev</category>
</item>
<item>
<title>Is Software Engineering Dead? 2026 Complete Career Guide</title>
<link>https://dev.to/dev_narratives_023afd008e/is-software-engineering-dead-2026-complete-career-guide-4588</link>
<guid>title:is software engineering dead 2026 complete career guide</guid>
<pubDate>Wed, 18 Feb 2026 08:41:24 +0000</pubDate>
<description>Software Engineering 2026: The Complete Guide to AI Survival By early 2026, 41% of all production code is generated by AI agents. Meanwhile, junior developer job postings have plummeted by 40% compared to 2022. It feels like the end of an era. But is the human coder actually extinct? Or are we standing at the edge of the most high-leverage period in software history? For those of us in the trenches of engineering and product management, the landscape has shifted beneath our feet.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>aicodingagents2026</category>
<category>ainativeidetools</category>
<category>dev.to</category>
<category>futureofsoftwareengineering</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>softwaredeveloperjobmarket</category>
<category>vision</category>
</item>
<item>
<title>How I Vibecoded a Sovereign Agent (And Accidentally Built an Enterprise Architecture)</title>
<link>https://dev.to/nemesiswave/how-i-vibecoded-a-sovereign-agent-and-accidentally-built-an-enterprise-architecture-1nok</link>
<guid>title:how i vibecoded a sovereign agent and accidentally built an enterprise architecture</guid>
<pubDate>Wed, 18 Feb 2026 08:33:30 +0000</pubDate>
<description>With the advent of AI, we all know how lazy mostly engineers have become to just vibecode half of the time and trust me it has rather improved efficiency in the corporate world. Thus, making employees feel like they shot themselves in the foot. But anyways we are in a constant cycle of evolution, so this is just a step for us engineers to become lazier and lazier, like who's going to keep revising syntax over and over again, right? When I know what I want to do let my chores be handled by AI. I vibecode half my day away. Honestly?</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>dev.to</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>programming</category>
<category>rl</category>
<category>vibecoding</category>
<category>vision</category>
<category>webdev</category>
</item>
<item>
<title>Top 10 OpenClaw Development Patterns and Architecture Best Practices</title>
<link>https://dev.to/chx381/top-10-openclaw-development-patterns-and-architecture-best-practices-37gp</link>
<guid>title:top 10 openclaw development patterns and architecture best practices</guid>
<pubDate>Wed, 18 Feb 2026 08:03:25 +0000</pubDate>
<description>Top 10 OpenClaw Development Patterns and Architecture Best Practices Building scalable, maintainable OpenClaw applications requires understanding proven development patterns and architectural best practices. This article explores the top patterns used by successful OpenClaw projects in 2026. 1. Core OpenClaw Architecture ⭐ 202,712 Repository : https://github. com/openclaw/openclaw Architecture Pattern : Modular Monolith with Microservices Ready // Core OpenClaw Architecture interface OpenClawCore { // Skill Management skillManager : SkillManager ; // Memory System memory : IMemoryBackend ; // Tool Framework tools : ToolRegistry ; // Event Bus eventBus : EventEmitter ; // Configuration config : ConfigManager ; } // Implementation class OpenClawInstance implements OpenClawCore { constructor ( config : Config ) { this . skillManager = new SkillManager (); this .</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>architecture</category>
<category>bestpractices</category>
<category>dev.to</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>openclaw</category>
<category>patterns</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>AXRP Episode 48 - Guive Assadi on AI Property Rights</title>
<link>https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights</link>
<guid>doi:10.1007/s00146-015-0590-y</guid>
<pubDate>Sun, 15 Feb 2026 02:20:55 +0000</pubDate>
<description>Published on February 15, 2026 2:20 AM GMT YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment?</description>
<source url="https://www.alignmentforum.org/feed.xml">alignment-forum</source>
<category>agents</category>
<category>alignment</category>
<category>alignment-forum</category>
<category>governance</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>robotics</category>
<category>safety</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>What OpenClaw’s Security Disasters Teach Us About the Future of AI Agents</title>
<link>https://pub.towardsai.net/what-openclaws-security-disasters-teach-us-about-the-future-of-ai-agents-ddd89e4228ec?source=rss----98111c9905da---4</link>
<guid>title:what openclaw s security disasters teach us about the future of ai agents</guid>
<pubDate>Wed, 18 Feb 2026 05:04:16 +0000</pubDate>
<description>100K GitHub stars. Thousands of exposed servers. A $16M crypto scam. The age of autonomous AI agents is here — and nobody is ready. In January 2026, a weekend project by Austrian developer Peter Steinberger broke the internet. OpenClaw (originally called Clawdbot) — a self-hosted AI agent that lives in your WhatsApp, Telegram, and Slack — racked up 9,000 GitHub stars in 24 hours and crossed 100,000 in under two months.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agents</category>
<category>ai</category>
<category>ai-agent</category>
<category>artificial-intelligence</category>
<category>cybersecurity</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>open-source</category>
<category>pub.towardsai.net</category>
<category>reasoning</category>
<category>vision</category>
</item>
<item>
<title>AI’s Diminishing Returns  : Avoiding the Overreliance Trap in BFSI</title>
<link>https://pub.towardsai.net/ais-diminishing-returns-the-financial-industry-s-over-reliance-trap-5084fc8bca67?source=rss----98111c9905da---4</link>
<guid>title:ai s diminishing returns avoiding the overreliance trap in bfsi</guid>
<pubDate>Wed, 18 Feb 2026 04:58:55 +0000</pubDate>
<description>AI’s Diminishing Returns: Avoiding the Overreliance Trap “I find it hard to see, how there can be a good return on investment given, the current math. ” This warning about AI infrastructure economics is not just for tech investors, but it has a direct bearing on banking and financial services, where the current wave of AI adoption risks building systemic fragility under the banner of innovation. Unlike earlier generation of AI solutions, which were often developed in-house, fine-tuned on proprietary datasets, and deployed within the secure boundaries of a bank’s own infrastructure, today’s wave of general-purpose AI is of a very different breed. At the centre of this shift are large language models ( LLMs) and similar foundation models, which are not built or run locally but unleashed at scale through centralised data centres and software stacks maintained by a handful of technology providers. Every solution built on such infrastructure, be it a chatbot, a fraud detection engine, or a compliance tool, ultimately depends on continuous calls to these central data centres. The implication is that future pricing models will inevitably evolve toward a per-transaction basis, much like how the industry already prices microservices and API-based services.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>ai</category>
<category>artificial-intelligence</category>
<category>economics</category>
<category>leadership</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>rl</category>
<category>robotics</category>
<category>serving</category>
<category>technology</category>
</item>
<item>
<title>Customer Impersonation Detection using LLM</title>
<link>https://pub.towardsai.net/customer-impersonation-detection-using-llm-cffa56ed58c1?source=rss----98111c9905da---4</link>
<guid>title:customer impersonation detection using llm</guid>
<pubDate>Wed, 18 Feb 2026 04:57:02 +0000</pubDate>
<description>Detecting Customer Fraud in Merchant Space Q1. What was the business usecase? In merchant space there are different types of fraud that can happen like: Fraudsters impersonating the platform (e. g. , pretending to be support) Fraudsters contacting merchants via phone/SMS. Social engineering attempts(revealing confidential info) to steal money, OTPs, credentials Suspicious phone numbers used in impersonation scams Q2.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agents</category>
<category>ai</category>
<category>artificial-intelligence</category>
<category>data-science</category>
<category>llm</category>
<category>machine-learning</category>
<category>medium</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>rl</category>
<category>serving</category>
<category>towards-data-science</category>
</item>
<item>
<title>Qwen3.5: Towards Native Multimodal Agents</title>
<link>https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything</link>
<guid>title:qwen3 5 towards native multimodal agents</guid>
<pubDate>Tue, 17 Feb 2026 04:30:57 +0000</pubDate>
<description>Qwen3. 5: Towards Native Multimodal Agents Alibaba's Qwen just released the first two models in the Qwen 3. 5 series - one open weights, one proprietary. Both are multi-modal for vision input. The open weight one is a Mixture of Experts model called Qwen3. 5-397B-A17B.</description>
<source url="https://simonwillison.net/atom/everything/">simonwillison</source>
<category>agents</category>
<category>ai</category>
<category>ai-in-china</category>
<category>engineering</category>
<category>generative-ai</category>
<category>llm</category>
<category>llm-release</category>
<category>llms</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>openrouter</category>
<category>pelican-riding-a-bicycle</category>
<category>qwen</category>
<category>serving</category>
<category>simonwillison</category>
<category>tools</category>
<category>vision</category>
<category>vision-llms</category>
</item>
<item>
<title>Kimi K2.5: Visual Agentic Intelligence â</title>
<link>https://www.kimi.com/blog/kimi-k2-5.html</link>
<guid>title:kimi k2 5 visual agentic intelligence</guid>
<pubDate>Wed, 18 Feb 2026 08:59:45 +0000</pubDate>
<description>Today, we are introducing Kimi K2. 5, the most powerful open-source model to date. Kimi K2. 5 builds on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens. Built as a native multimodal model, K2. 5 delivers state-of-the-art coding and vision capabilities and a self-directed agent swarm paradigm.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>agents</category>
<category>kimi</category>
<category>lab</category>
<category>models</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>product</category>
<category>vision</category>
</item>
<item>
<title>Project Vend: Phase two</title>
<link>https://www.anthropic.com/research/project-vend-2</link>
<guid>title:project vend phase two</guid>
<pubDate>Wed, 18 Feb 2026 08:59:35 +0000</pubDate>
<description>In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend , a free-form experiment exploring how well AIs could do on complex, real-world tasks. Alas, the shopkeeper—a modified version of Claude we named “Claudius”—did not do particularly well. It lost money over time, had a strange identity crisis where it claimed it was a human wearing a blue blazer, and was goaded by mischievous Anthropic employees into selling products (particularly, for some reason, tungsten cubes) at a substantial loss. But the capabilities of large language models in areas like reasoning, writing, coding, and much else besides are increasing at a breathless pace. Has Claudius’s “running a shop” capability shown the same improvement?</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>#321 Nick Frosst: Why Cohere Is Betting on Enterprise AI, Not AGI</title>
<link>https://aneyeonai.libsyn.com/321-nick-frosst-why-cohere-is-betting-on-enterprise-ai-not-agi</link>
<guid>title:321 nick frosst why cohere is betting on enterprise ai not agi</guid>
<pubDate>Tue, 17 Feb 2026 21:00:00 +0000</pubDate>
<description>This episode is sponsored by tastytrade. Trade stocks, options, futures, and crypto in one platform with low commissions and zero commission on stocks and crypto. Built for traders who think in probabilities, tastytrade offers advanced analytics, risk tools, and an AI-powered Search feature. Learn more at https://tastytrade. com/ In this episode of Eye on AI, Nick Frosst, Co-Founder of Cohere and former Google Brain researcher, explains why Cohere is betting on enterprise AI instead of chasing AGI. While much of the AI industry is focused on artificial general intelligence, Cohere is building practical, capital-efficient large language models designed for real-world enterprise deployment.</description>
<source url="https://aneyeonai.libsyn.com/rss">aneyeonai.libsyn.com</source>
<category>agents</category>
<category>ai</category>
<category>aneyeonai.libsyn.com</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>Qwen 3.5 The GREATEST Opensource AI Model That Beats Opus 4.5 and Gemini 3? (Fully Tested)</title>
<link>https://www.youtube.com/watch?v=TgZVAYXteIs</link>
<guid>title:qwen 3 5 the greatest opensource ai model that beats opus 4 5 and gemini 3 fully tested</guid>
<pubDate>Tue, 17 Feb 2026 08:01:08 +0000</pubDate>
<description>📢 Access top AI models and creators like Anthropic’s Claude, OpenAI’s GPT, Meta’s Llama, DeepSeek, Moonshot AI’s Kimi, plus image generation with Black Forest Labs (Flux) and Recraft — all in one place with Mammouth starting at just $10/month: https://mammouth. ai The Alibaba team is back with a massive open-weight flagship AI — introducing the Qwen 3. 5 series! This 397B parameter model (17B active) is natively multimodal, designed for agents, coding, browsing, and multimodal tasks, and it’s reportedly 19x faster than Qwen3 Max. 🔗 My Links: Sponsor a Video or Do a Demo of Your Product, Contact me: intheworldzofai@gmail. com 🔥 Become a Patron (Private Discord): https://patreon.</description>
<source url="https://www.youtube.com/feeds/videos.xml?channel_id=UC2WmuBuFq6gL08QYG-JjXKw">youtube.com</source>
<category>agents</category>
<category>ai</category>
<category>llm</category>
<category>multimodal</category>
<category>news</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>youtube.com</category>
</item>
<item>
<title>China’s dancing robots: how worried should we be?</title>
<link>https://www.theguardian.com/world/2026/feb/18/china-dancing-humanoid-robots-festival-show</link>
<guid>title:china s dancing robots how worried should we be</guid>
<pubDate>Wed, 18 Feb 2026 05:19:09 +0000</pubDate>
<description>Eye-catching martial arts performance at China gala had viewers and experts wondering what else humanoids can do Dancing humanoid robots took centre stage on Monday during the annual China Media Group’s Spring Festival Gala, China’s most-watched official television broadcast. They lunged and backflipped (landing on their knees), they spun around and jumped. Not one fell over. The display was impressive, but prompted some to wonder: if robots can now dance and perform martial arts, what else can they do? Continue reading...</description>
<source url="https://www.theguardian.com/technology/artificialintelligenceai/rss">theguardian.com</source>
<category>ai</category>
<category>ai (artificial intelligence)</category>
<category>asia pacific</category>
<category>china</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>robotics</category>
<category>robots</category>
<category>technology</category>
<category>theguardian.com</category>
<category>vision</category>
<category>world news</category>
</item>
<item>
<title>Cloudflare Releases Agents SDK v0.5.0 with Rewritten @cloudflare/ai-chat and New Rust-Powered Infire Engine for Optimized Edge Inference Performance</title>
<link>https://www.marktechpost.com/2026/02/17/cloudflare-releases-agents-sdk-v0-5-0-with-rewritten-cloudflare-ai-chat-and-new-rust-powered-infire-engine-for-optimized-edge-inference-performance/</link>
<guid>title:cloudflare releases agents sdk v0 5 0 with rewritten cloudflare ai chat and new rust powered infire engine for optimized edge inference performance</guid>
<pubDate>Tue, 17 Feb 2026 18:04:47 +0000</pubDate>
<description>Cloudflare has released the Agents SDK v0. 5. 0 to address the limitations of stateless serverless functions in AI development. In standard serverless architectures, every LLM call requires rebuilding the session context from scratch, which increases latency and token consumption. The Agents SDK&amp;#8217;s latest version (Agents SDK v0. 5.</description>
<source url="https://www.marktechpost.com/feed">marktechpost.com</source>
<category>agentic ai</category>
<category>agents</category>
<category>ai</category>
<category>ai agents</category>
<category>editors pick</category>
<category>llm</category>
<category>marktechpost.com</category>
<category>new releases</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>serving</category>
<category>staff</category>
</item>
<item>
<title>Moonshot AI Releases Open-Weight Kimi K2.5 Model with Vision and Agent Swarm Capabilities</title>
<link>https://www.infoq.com/news/2026/02/kimi-k25-swarm/?utm_campaign=infoq_content&amp;utm_source=infoq&amp;utm_medium=feed&amp;utm_term=AI%2C+ML+%26+Data+Engineering</link>
<guid>title:moonshot ai releases open weight kimi k2 5 model with vision and agent swarm capabilities</guid>
<pubDate>Tue, 17 Feb 2026 14:00:00 +0000</pubDate>
<description>Moonshot AI released Kimi K2.5, their latest open-weight multimodal LLM. K2.5 excels at coding tasks, with benchmark scores comparable to frontier models such as GPT-5 and Gemini. It also features an agent swarm mode, which can direct up to 100 sub-agents for attacking problems with parallel workflow. By Anthony Alford</description>
<source url="https://feed.infoq.com/ai-ml-data-eng/">feed.infoq.com</source>
<category>agents</category>
<category>ai</category>
<category>ai, ml &amp; data engineering</category>
<category>feed.infoq.com</category>
<category>llm</category>
<category>multimodal</category>
<category>news</category>
<category>nonpaper</category>
<category>vision</category>
</item>
<item>
<title>Kimi Introduces Agent Swarm: Let 100 AI Agents Work for You â</title>
<link>https://www.kimi.com/blog/agent-swarm.html</link>
<guid>title:kimi introduces agent swarm let 100 ai agents work for you</guid>
<pubDate>Wed, 18 Feb 2026 08:59:43 +0000</pubDate>
<description>In 2025, if you walked into any AI conference, you may hear the same gospel: faster inference, longer context windows, cheaper inference costs. It's as if we've spent years perfecting the hammer, making it lighter, stronger, more precisely balanced, while never questioning the fact that the carpenter still has only two hands and twenty-four hours in a day. Now, Kimi introduces Agent Swarm. It is not a better hammer. It is a reconstruction of the entire workshop.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>agents</category>
<category>kimi</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>product</category>
<category>serving</category>
</item>
</channel>
</rss>