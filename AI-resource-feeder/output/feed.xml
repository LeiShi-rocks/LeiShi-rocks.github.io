<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Trend Feed</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers/blogs ranked by recency + popularity + relevance</description>
<lastBuildDate>Fri, 20 Feb 2026 08:55:59 +0000</lastBuildDate>
<item>
<title>AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games</title>
<link>https://tldr.takara.ai/p/2602.17594</link>
<guid>title:ai gamestore scalable open ended evaluation of machine general intelligence with human games</guid>
<pubDate>Thu, 19 Feb 2026 18:17:25 +0000</pubDate>
<description>Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a &quot;human game&quot; to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the &quot;Multiverse of Human Games&quot;. Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>vision</category>
<category>world-models</category>
</item>
<item>
<title>OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</title>
<link>https://tldr.takara.ai/p/2602.17665</link>
<guid>title:openearthagent a unified framework for tool augmented geospatial agents</guid>
<pubDate>Thu, 19 Feb 2026 18:59:54 +0000</pubDate>
<description>Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>Sink-Aware Pruning for Diffusion Language Models</title>
<link>https://tldr.takara.ai/p/2602.17664</link>
<guid>title:sink aware pruning for diffusion language models</guid>
<pubDate>Thu, 19 Feb 2026 18:59:50 +0000</pubDate>
<description>Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>FAMOSE: A ReAct Approach to Automated Feature Discovery</title>
<link>https://tldr.takara.ai/p/2602.17641</link>
<guid>title:famose a react approach to automated feature discovery</guid>
<pubDate>Thu, 19 Feb 2026 18:53:15 +0000</pubDate>
<description>Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0. 23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2. 0% on average, while remaining more robust to errors than other algorithms.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Unmasking the Factual-Conceptual Gap in Persian Language Models</title>
<link>https://tldr.takara.ai/p/2602.17623</link>
<guid>title:unmasking the factual conceptual gap in persian language models</guid>
<pubDate>Thu, 19 Feb 2026 18:42:46 +0000</pubDate>
<description>While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Towards Anytime-Valid Statistical Watermarking</title>
<link>https://tldr.takara.ai/p/2602.17608</link>
<guid>title:towards anytime valid statistical watermarking</guid>
<pubDate>Thu, 19 Feb 2026 18:32:26 +0000</pubDate>
<description>The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>A Theoretical Framework for Modular Learning of Robust Generative Models</title>
<link>https://tldr.takara.ai/p/2602.17554</link>
<guid>title:a theoretical framework for modular learning of robust generative models</guid>
<pubDate>Thu, 19 Feb 2026 17:09:13 +0000</pubDate>
<description>Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities</title>
<link>https://tldr.takara.ai/p/2602.17402</link>
<guid>title:a contrastive variational autoencoder for nsclc survival prediction with missing modalities</guid>
<pubDate>Thu, 19 Feb 2026 14:29:34 +0000</pubDate>
<description>Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Anthropic and Infosys collaborate to build AI agents for telecommunications and other regulated industries</title>
<link>https://www.anthropic.com/news/anthropic-infosys</link>
<guid>title:anthropic and infosys collaborate to build ai agents for telecommunications and other regulated industries</guid>
<pubDate>Fri, 20 Feb 2026 08:55:47 +0000</pubDate>
<description>Anthropic and Infosys , a global leader in next-generation digital services and consulting founded and headquartered in Bengaluru, today announced a collaboration to develop and deliver enterprise AI solutions across telecommunications, financial services, manufacturing, and software development. The collaboration integrates Anthropic’s Claude models and Claude Code with Infosys Topaz , an AI-first set of services, solutions, and platforms using generative and agentic AI technologies, to help companies speed up software development and adopt AI with the governance and transparency that regulated industries require. India is the second-largest market for Claude.ai, home to a developer community doing some of the most technically intense AI work we see anywhere: nearly half of Claude usage in India involves building applications, modernizing systems, and shipping production software. Infosys is one of the first partners in Anthropic’s expanded presence in India .</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>MARS: Margin-Aware Reward-Modeling with Self-Refinement</title>
<link>https://tldr.takara.ai/p/2602.17658</link>
<guid>title:mars margin aware reward modeling with self refinement</guid>
<pubDate>Thu, 19 Feb 2026 18:59:03 +0000</pubDate>
<description>Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</title>
<link>https://tldr.takara.ai/p/2602.17654</link>
<guid>title:mine and refine optimizing graded relevance in e commerce search retrieval</guid>
<pubDate>Thu, 19 Feb 2026 18:56:36 +0000</pubDate>
<description>We propose a two-stage &quot;Mine and Refine&quot; contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</title>
<link>https://tldr.takara.ai/p/2602.17605</link>
<guid>title:adapting actively on the fly relevance guided online meta learning with latent concepts for geospatial discovery</guid>
<pubDate>Thu, 19 Feb 2026 18:30:18 +0000</pubDate>
<description>In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e. g. , land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction</title>
<link>https://tldr.takara.ai/p/2602.17102</link>
<guid>title:operationalization of machine learning with serverless architecture an industrial operationalization of machine learning with serverless architecture an industrial implementation for harmonized system code prediction</guid>
<pubDate>Thu, 19 Feb 2026 05:59:55 +0000</pubDate>
<description>This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>Probability-Invariant Random Walk Learning on Gyral Folding-Based Cortical Similarity Networks for Alzheimer's and Lewy Body Dementia Diagnosis</title>
<link>https://tldr.takara.ai/p/2602.17557</link>
<guid>title:probability invariant random walk learning on gyral folding based cortical similarity networks for alzheimer s and lewy body dementia diagnosis</guid>
<pubDate>Thu, 19 Feb 2026 17:11:59 +0000</pubDate>
<description>Alzheimer's disease (AD) and Lewy body dementia (LBD) present overlapping clinical features yet require distinct diagnostic strategies. While neuroimaging-based brain network analysis is promising, atlas-based representations may obscure individualized anatomy. Gyral folding-based networks using three-hinge gyri provide a biologically grounded alternative, but inter-individual variability in cortical folding results in inconsistent landmark correspondence and highly irregular network sizes, violating the fixed-topology and node-alignment assumptions of most existing graph learning methods, particularly in clinical datasets where pathological changes further amplify anatomical heterogeneity. We therefore propose a probability-invariant random-walk-based framework that classifies individualized gyral folding networks without explicit node alignment. Cortical similarity networks are built from local morphometric features and represented by distributions of anonymized random walks, with an anatomy-aware encoding that preserves permutation invariance. Experiments on a large clinical cohort of AD and LBD subjects show consistent improvements over existing gyral folding and atlas-based models, demonstrating robustness and potential for dementia diagnosis.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs</title>
<link>https://tldr.takara.ai/p/2602.17535</link>
<guid>title:lata laplacian assisted transductive adaptation for conformal uncertainty in medical vlms</guid>
<pubDate>Thu, 19 Feb 2026 16:45:38 +0000</pubDate>
<description>Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>vision</category>
</item>
<item>
<title>What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data</title>
<link>https://tldr.takara.ai/p/2602.17483</link>
<guid>title:what do llms associate with your name a human centered black box audit of personal data</guid>
<pubDate>Thu, 19 Feb 2026 15:53:29 +0000</pubDate>
<description>Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e. g.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Flickering Multi-Armed Bandits</title>
<link>https://tldr.takara.ai/p/2602.17315</link>
<guid>title:flickering multi armed bandits</guid>
<pubDate>Thu, 19 Feb 2026 12:24:01 +0000</pubDate>
<description>We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i. i. d. Erdős--Rényi (ER) process and an Edge-Markovian process.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>robotics</category>
</item>
<item>
<title>Anthropic opens Bengaluru office and announces new partnerships across India</title>
<link>https://www.anthropic.com/news/bengaluru-office-partnerships-across-india</link>
<guid>title:anthropic opens bengaluru office and announces new partnerships across india</guid>
<pubDate>Fri, 20 Feb 2026 08:55:49 +0000</pubDate>
<description>India is the second-largest market for Claude. ai , home to a developer community doing some of the most technically intense AI work we see anywhere. Nearly half of Claude usage in India comprises computer and mathematical tasks: building applications, modernizing systems, and shipping production software. Today, as we officially open our Bengaluru office, we’re announcing partnerships across enterprise, education, and agriculture that deepen our commitment to India across a range of sectors. “India represents one of the world’s most promising opportunities to bring the benefits of responsible AI to vastly more people and enterprises,” said Irina Ghose, Managing Director of India, Anthropic. “Already, it’s home to extraordinary technical talent, digital infrastructure at scale, and a proven track record of using technology to improve people’s lives.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Anthropic raises $30 billion in Series G funding at $380 billion post-money valuation</title>
<link>https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation</link>
<guid>title:anthropic raises 30 billion in series g funding at 380 billion post money valuation</guid>
<pubDate>Fri, 20 Feb 2026 08:55:44 +0000</pubDate>
<description>We have raised $30 billion in Series G funding led by GIC and Coatue, valuing Anthropic at $380 billion post-money. The round was co-led by D. E. Shaw Ventures, Dragoneer, Founders Fund, ICONIQ, and MGX. The investment will fuel the frontier research, product development, and infrastructure expansions that have made Anthropic the market leader in enterprise AI and coding. Significant investors in this round include: Accel, Addition, Alpha Wave Global, Altimeter, AMP PBC, Appaloosa LP, Baillie Gifford, Bessemer Venture Partners, affiliated funds of BlackRock, Blackstone, D1 Capital Partners, Fidelity Management &amp; Research Company, General Catalyst, Greenoaks, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, JPMorganChase through its Security and Resiliency Initiative and Growth Equity Partners, Lightspeed Venture Partners, Menlo Ventures, Morgan Stanley Investment Management, NX1 Capital, Qatar Investment Authority (QIA), Sands Capital, Sequoia Capital, Temasek, TowerBrook, TPG, Whale Rock Capital, and XN.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Fri, 20 Feb 2026 08:55:40 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta 1 .</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Fri, 20 Feb 2026 08:55:37 +0000</pubDate>
<description>Sonnet 4. 6 delivers frontier performance across coding, agents, and professional work at scale. We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Continual uncertainty learning</title>
<link>https://tldr.takara.ai/p/2602.17174</link>
<guid>title:continual uncertainty learning</guid>
<pubDate>Thu, 19 Feb 2026 08:39:42 +0000</pubDate>
<description>Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning</title>
<link>https://tldr.takara.ai/p/2602.17168</link>
<guid>title:badclip stealthy and persistent backdoors in multimodal contrastive learning</guid>
<pubDate>Thu, 19 Feb 2026 08:31:16 +0000</pubDate>
<description>Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</title>
<link>https://tldr.takara.ai/p/2602.17663</link>
<guid>title:clef hipe 2026 evaluating accurate and efficient person place relation extraction from multilingual historical texts</guid>
<pubDate>Thu, 19 Feb 2026 18:59:44 +0000</pubDate>
<description>HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (&quot;Has the person ever been at this place? &quot;) and $isAt$ (&quot;Is the person located at this place around publication time? &quot;) - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
</item>
<item>
<title>SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</title>
<link>https://tldr.takara.ai/p/2602.17632</link>
<guid>title:smac score matched actor critics for robust offline to online transfer</guid>
<pubDate>Thu, 19 Feb 2026 18:47:31 +0000</pubDate>
<description>Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Revisiting Weight Regularization for Low-Rank Continual Learning</title>
<link>https://tldr.takara.ai/p/2602.17559</link>
<guid>title:revisiting weight regularization for low rank continual learning</guid>
<pubDate>Thu, 19 Feb 2026 17:13:00 +0000</pubDate>
<description>Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Adaptive Decentralized Composite Optimization via Three-Operator Splitting</title>
<link>https://tldr.takara.ai/p/2602.17545</link>
<guid>title:adaptive decentralized composite optimization via three operator splitting</guid>
<pubDate>Thu, 19 Feb 2026 16:59:34 +0000</pubDate>
<description>The paper studies decentralized optimization over networks, where agents minimize a sum of {\it locally} smooth (strongly) convex losses and plus a nonsmooth convex extended value term. We propose decentralized methods wherein agents {\it adaptively} adjust their stepsize via local backtracking procedures coupled with lightweight min-consensus protocols. Our design stems from a three-operator splitting factorization applied to an equivalent reformulation of the problem. The reformulation is endowed with a new BCV preconditioning metric (Bertsekas-O'Connor-Vandenberghe), which enables efficient decentralized implementation and local stepsize adjustments. We establish robust convergence guarantees. Under mere convexity, the proposed methods converge with a sublinear rate.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses</title>
<link>https://tldr.takara.ai/p/2602.17084</link>
<guid>title:how ai coding agents communicate a study of pull request description characteristics and human review responses</guid>
<pubDate>Thu, 19 Feb 2026 05:06:31 +0000</pubDate>
<description>The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Position: Evaluation of ECG Representations Must Be Fixed</title>
<link>https://tldr.takara.ai/p/2602.17531</link>
<guid>title:position evaluation of ecg representations must be fixed</guid>
<pubDate>Thu, 19 Feb 2026 16:42:46 +0000</pubDate>
<description>This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics</title>
<link>https://tldr.takara.ai/p/2602.17513</link>
<guid>title:bridging the domain divide supervised vs zero shot clinical section segmentation from mimic iii to obstetrics</guid>
<pubDate>Thu, 19 Feb 2026 16:25:07 +0000</pubDate>
<description>Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Learning with Boolean threshold functions</title>
<link>https://tldr.takara.ai/p/2602.17493</link>
<guid>title:learning with boolean threshold functions</guid>
<pubDate>Thu, 19 Feb 2026 16:07:25 +0000</pubDate>
<description>We develop a method for training neural networks on Boolean data in which the values at all nodes are strictly $\pm 1$, and the resulting models are typically equivalent to networks whose nonzero weights are also $\pm 1$. The method replaces loss minimization with a nonconvex constraint formulation. Each node implements a Boolean threshold function (BTF), and training is expressed through a divide-and-concur decomposition into two complementary constraints: one enforces local BTF consistency between inputs, weights, and output; the other imposes architectural concurrence, equating neuron outputs with downstream inputs and enforcing weight equality across training-data instantiations of the network. The reflect-reflect-relax (RRR) projection algorithm is used to reconcile these constraints. Each BTF constraint includes a lower bound on the margin. When this bound is sufficiently large, the learned representations are provably sparse and equivalent to networks composed of simple logical gates with $\pm 1$ weights.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning</title>
<link>https://tldr.takara.ai/p/2602.17062</link>
<guid>title:retaining suboptimal actions to follow shifting optima in multi agent reinforcement learning</guid>
<pubDate>Thu, 19 Feb 2026 04:07:55 +0000</pubDate>
<description>Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Entropy-Based Data Selection for Language Models</title>
<link>https://tldr.takara.ai/p/2602.17465</link>
<guid>title:entropy based data selection for language models</guid>
<pubDate>Thu, 19 Feb 2026 15:29:34 +0000</pubDate>
<description>Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>ABCD: All Biases Come Disguised</title>
<link>https://tldr.takara.ai/p/2602.17445</link>
<guid>title:abcd all biases come disguised</guid>
<pubDate>Thu, 19 Feb 2026 15:12:33 +0000</pubDate>
<description>Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
</item>
<item>
<title>Anthropic and the Government of Rwanda sign MOU for AI in health and education</title>
<link>https://www.anthropic.com/news/anthropic-rwanda-mou</link>
<guid>title:anthropic and the government of rwanda sign mou for ai in health and education</guid>
<pubDate>Fri, 20 Feb 2026 08:55:45 +0000</pubDate>
<description>The Government of Rwanda and Anthropic have signed a three-year Memorandum of Understanding to formalize and expand our partnership, bringing AI to Rwanda’s education, health, and public sector systems. This agreement builds on the ALX education partnership we announced in November 2025 and marks the first time Anthropic has formalized a multi-sector partnership through a government MOU on the African continent. Our collaboration spans three areas: “This partnership with Anthropic is an important milestone in Rwanda’s AI journey. Our goal is to continue to design and deploy AI solutions that can be applied at a national level to strengthen education, advance health outcomes, and enhance governance with an emphasis on our context,” said Paula Ingabire, Minister of Information and Communications Technology (ICT) and Innovation in Rwanda.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Fri, 20 Feb 2026 08:55:43 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>How NVIDIA Extreme Hardware-Software Co-Design Delivered a Large Inference Boost for Sarvam AI’s Sovereign Models</title>
<link>https://developer.nvidia.com/blog/how-nvidia-extreme-hardware-software-co-design-delivered-a-large-inference-boost-for-sarvam-ais-sovereign-models/</link>
<guid>title:how nvidia extreme hardware software co design delivered a large inference boost for sarvam ai s sovereign models</guid>
<pubDate>Wed, 18 Feb 2026 16:00:00 +0000</pubDate>
<description>As global AI adoption accelerates, developers face a growing challenge: delivering large language model (LLM) performance that meets real-world latency and cost...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>data center / cloud</category>
<category>data science</category>
<category>h100</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>nemo</category>
<category>nemotron</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>nvidia inception</category>
<category>rl</category>
<category>serving</category>
<category>training</category>
</item>
<item>
<title>“No technology has me dreaming bigger than AI”</title>
<link>https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026/</link>
<guid>title:no technology has me dreaming bigger than ai</guid>
<pubDate>Thu, 19 Feb 2026 04:30:00 +0000</pubDate>
<description>a stylized design resembling the Ashoka Chakra with colorful network lines and text reading &quot;भारत 2026 INDIA.&quot; A vertical line separates it from the Google logo on the right, all set against a light blue gradient background with a faint grid pattern.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>a message from our ceo</category>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>AI Impact Summit 2026</title>
<link>https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection/</link>
<guid>title:ai impact summit 2026</guid>
<pubDate>Thu, 19 Feb 2026 04:30:00 +0000</pubDate>
<description>A look at the partnerships and investments Google announced at the AI Impact Summit 2026.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Accelerating Data Processing with NVIDIA Multi-Instance GPU and NUMA Node Localization</title>
<link>https://developer.nvidia.com/blog/accelerating-data-processing-with-nvidia-multi-instance-gpu-and-numa-node-localization/</link>
<guid>title:accelerating data processing with nvidia multi instance gpu and numa node localization</guid>
<pubDate>Thu, 19 Feb 2026 17:30:00 +0000</pubDate>
<description>NVIDIA flagship data center GPUs in the NVIDIA Ampere, NVIDIA Hopper, and NVIDIA Blackwell families all feature non-uniform memory access (NUMA) behaviors, but...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>cuda c++</category>
<category>data analytics / processing</category>
<category>data center / cloud</category>
<category>hardware</category>
<category>infra</category>
<category>memory</category>
<category>multi-instance gpu (mig)</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>simulation / modeling / design</category>
<category>training</category>
</item>
<item>
<title>A new way to express yourself: Gemini can now create music</title>
<link>https://blog.google/innovation-and-ai/products/gemini-app/lyria-3/</link>
<guid>title:a new way to express yourself gemini can now create music</guid>
<pubDate>Wed, 18 Feb 2026 16:00:00 +0000</pubDate>
<description>Image showing sample tracks created with Lyria 3</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini app</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>AI Impact Summit 2026: How we’re partnering to make AI work for everyone</title>
<link>https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india/</link>
<guid>title:ai impact summit 2026 how we re partnering to make ai work for everyone</guid>
<pubDate>Wed, 18 Feb 2026 10:30:00 +0000</pubDate>
<description>four people seated on a conference stage</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google in asia</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Lasso Security Introduces Intent Deputy</title>
<link>https://ai-techpark.com/lasso-security-introduces-intent-deputy/</link>
<guid>title:lasso security introduces intent deputy</guid>
<pubDate>Wed, 18 Feb 2026 18:00:00 +0000</pubDate>
<description>New product delivers on the company’s Intent Security vision with real-time behavioral analysis at sub-50ms speeds and 99.83% detection accuracy Lasso Security, the platform enabling secure AI adoption at enterprise scale, today launched Intent Deputy: the industry’s first behavioral intent framework purpose-built for securing AI agents. Intent Deputy is the foundational... The post Lasso Security Introduces Intent Deputy first appeared on AI-Tech Park .</description>
<source url="https://ai-techpark.com/category/ai/feed/">ai-techpark.com</source>
<category>agents</category>
<category>ai</category>
<category>ai adoption</category>
<category>ai news</category>
<category>ai tech news</category>
<category>ai technology news</category>
<category>ai trending news</category>
<category>ai-techpark.com</category>
<category>artificial intelligence news</category>
<category>chatbots</category>
<category>lasso security</category>
<category>news</category>
<category>nonpaper</category>
<category>vision</category>
</item>
<item>
<title>Our 2026 Responsible AI Progress Report</title>
<link>https://blog.google/innovation-and-ai/products/responsible-ai-2026-report-ongoing-work/</link>
<guid>title:our 2026 responsible ai progress report</guid>
<pubDate>Tue, 17 Feb 2026 22:30:00 +0000</pubDate>
<description>an illustration of blue and white cubes</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Teaching AI to read a map</title>
<link>https://research.google/blog/teaching-ai-to-read-a-map/</link>
<guid>title:teaching ai to read a map</guid>
<pubDate>Tue, 17 Feb 2026 21:37:00 +0000</pubDate>
<description>Machine Perception</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>google-research</category>
<category>machine perception</category>
<category>models</category>
<category>nonpaper</category>
<category>open source models &amp; datasets</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>Build AI-Ready Knowledge Systems Using 5 Essential Multimodal RAG Capabilities</title>
<link>https://developer.nvidia.com/blog/build-ai-ready-knowledge-systems-using-5-essential-multimodal-rag-capabilities/</link>
<guid>title:build ai ready knowledge systems using 5 essential multimodal rag capabilities</guid>
<pubDate>Tue, 17 Feb 2026 18:00:00 +0000</pubDate>
<description>Enterprise data is inherently complex: real-world documents are multimodal, spanning text, tables, charts and graphs, images, diagrams, scanned pages, forms,...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai agent</category>
<category>data center / cloud</category>
<category>hardware</category>
<category>infra</category>
<category>llms</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>retrieval augmented generation (rag)</category>
<category>rl</category>
<category>training</category>
</item>
<item>
<title>Unlock Massive Token Throughput with GPU Fractioning in NVIDIA Run:ai</title>
<link>https://developer.nvidia.com/blog/unlock-massive-token-throughput-with-gpu-fractioning-in-nvidia-runai/</link>
<guid>title:unlock massive token throughput with gpu fractioning in nvidia run ai</guid>
<pubDate>Wed, 18 Feb 2026 18:00:00 +0000</pubDate>
<description>As AI workloads scale, achieving high throughput, efficient resource usage, and predictable latency becomes essential. NVIDIA Run:ai addresses these challenges...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai inference</category>
<category>data center / cloud</category>
<category>data science</category>
<category>hardware</category>
<category>inference performance</category>
<category>infra</category>
<category>llms</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Accelerating discovery in India through AI-powered science and education</title>
<link>https://deepmind.google/blog/accelerating-discovery-in-india-through-ai-powered-science-and-education/</link>
<guid>title:accelerating discovery in india through ai powered science and education</guid>
<pubDate>Tue, 17 Feb 2026 13:42:20 +0000</pubDate>
<description>Google DeepMind brings National Partnerships for AI initiative to India, scaling AI for science and education</description>
<source url="https://deepmind.google/blog/feed/basic/">deepmind</source>
<category>deepmind</category>
<category>lab</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>Topping the GPU MODE Kernel Leaderboard with NVIDIA cuda.compute</title>
<link>https://developer.nvidia.com/blog/topping-the-gpu-mode-kernel-leaderboard-with-nvidia-cuda-compute/</link>
<guid>title:topping the gpu mode kernel leaderboard with nvidia cuda compute</guid>
<pubDate>Wed, 18 Feb 2026 17:00:00 +0000</pubDate>
<description>Python dominates machine learning for its ergonomics, but writing truly fast GPU code has historically meant dropping into C++ to write custom kernels and to...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>cuda</category>
<category>data science</category>
<category>developer tools &amp; techniques</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Lumen Unveils Gateway to Ease AI Network Strain</title>
<link>https://ai-techpark.com/lumen-unveils-gateway-to-ease-ai-network-strain/</link>
<guid>title:lumen unveils gateway to ease ai network strain</guid>
<pubDate>Wed, 18 Feb 2026 09:15:00 +0000</pubDate>
<description>New enterprise capabilities are designed to accelerate data movement across distributed AI environments while lowering complexity and total cost As AI drives explosive data growth, enterprises are under increasing pressure to move information quickly and securely across clouds, data centers, and distributed locations. In response, Lumen Technologies (NYSE: LUMN) today... The post Lumen Unveils Gateway to Ease AI Network Strain first appeared on AI-Tech Park .</description>
<source url="https://ai-techpark.com/category/ai/feed/">ai-techpark.com</source>
<category>ai</category>
<category>ai news</category>
<category>ai tech news</category>
<category>ai technology news</category>
<category>ai trending news</category>
<category>ai-techpark.com</category>
<category>artificial intelligence news</category>
<category>data centers</category>
<category>news</category>
<category>nonpaper</category>
</item>
<item>
<title>Scheduling in a changing world: Maximizing throughput with time-varying capacity</title>
<link>https://research.google/blog/scheduling-in-a-changing-world-maximizing-throughput-with-time-varying-capacity/</link>
<guid>title:scheduling in a changing world maximizing throughput with time varying capacity</guid>
<pubDate>Wed, 11 Feb 2026 10:34:00 +0000</pubDate>
<description>Algorithms &amp; Theory</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>algorithms &amp; theory</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>rl</category>
<category>science</category>
</item>
<item>
<title>Gemini 3 Deep Think: Advancing science, research and engineering</title>
<link>https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/</link>
<guid>title:gemini 3 deep think advancing science research and engineering</guid>
<pubDate>Thu, 12 Feb 2026 16:13:00 +0000</pubDate>
<description>Gemini 3 Deep Think logo</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini models</category>
<category>google</category>
<category>google deepmind</category>
<category>google one</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>R²D²: Scaling Multimodal Robot Learning with NVIDIA Isaac Lab</title>
<link>https://developer.nvidia.com/blog/r2d2-scaling-multimodal-robot-learning-with-nvidia-isaac-lab/</link>
<guid>title:r d scaling multimodal robot learning with nvidia isaac lab</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Building robust, intelligent robots requires testing them in complex environments. However, gathering data in the physical world is expensive, slow, and often...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>ai foundation models</category>
<category>hardware</category>
<category>humanoid robots</category>
<category>infra</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>nvidia research</category>
<category>open source</category>
<category>physical ai</category>
<category>rl</category>
<category>robotics</category>
<category>robotics research and development digest (r²d²)</category>
<category>simulation / modeling / design</category>
<category>training</category>
</item>
<item>
<title>Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations</title>
<link>https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/</link>
<guid>title:beyond one on one authoring simulating and testing dynamic human ai group conversations</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Human-Computer Interaction and Visualization</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>google-research</category>
<category>human-computer interaction and visualization</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>9 fun questions to try asking Google Photos</title>
<link>https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/</link>
<guid>title:9 fun questions to try asking google photos</guid>
<pubDate>Tue, 10 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of outdoor images, a blue icon that say &quot;Ask Photos,&quot; and examples of Ask Photos prompts.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Helping kids and teens learn and grow online on Safer Internet Day</title>
<link>https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/</link>
<guid>title:helping kids and teens learn and grow online on safer internet day</guid>
<pubDate>Tue, 10 Feb 2026 02:30:00 +0000</pubDate>
<description>User profile on smartphone connected to security, media, and settings icons.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>safety &amp; security</category>
</item>
<item>
<title>Automating Inference Optimizations with NVIDIA TensorRT LLM AutoDeploy</title>
<link>https://developer.nvidia.com/blog/automating-inference-optimizations-with-nvidia-tensorrt-llm-autodeploy/</link>
<guid>title:automating inference optimizations with nvidia tensorrt llm autodeploy</guid>
<pubDate>Mon, 09 Feb 2026 18:30:00 +0000</pubDate>
<description>NVIDIA TensorRT LLM enables developers to build high-performance inference engines for large language models (LLMs), but deploying a new architecture...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai inference</category>
<category>developer tools &amp; techniques</category>
<category>hardware</category>
<category>inference performance</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>mlops</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>pytorch</category>
<category>serving</category>
<category>training</category>
</item>
<item>
<title>How AI trained on birds is surfacing underwater mysteries</title>
<link>https://research.google/blog/how-ai-trained-on-birds-is-surfacing-underwater-mysteries/</link>
<guid>title:how ai trained on birds is surfacing underwater mysteries</guid>
<pubDate>Mon, 09 Feb 2026 18:38:00 +0000</pubDate>
<description>Climate &amp; Sustainability</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>climate &amp; sustainability</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>open source models &amp; datasets</category>
<category>research</category>
<category>science</category>
<category>sound &amp; accoustics</category>
</item>
<item>
<title>Anthropic AI safety researcher quits, says the ‘world is in peril’</title>
<link>https://globalnews.ca/news/11664538/anthropic-ai-safety-researcher-mrinank-sharma-quits-concerns/</link>
<guid>title:anthropic ai safety researcher quits says the world is in peril</guid>
<pubDate>Thu, 12 Feb 2026 17:00:46 +0000</pubDate>
<description>Anthropic was founded in 2021 by a breakaway group of former OpenAI employees who pledged to design a more safety-centric approach to AI development.</description>
<source url="https://globalnews.ca/tag/artificial-intelligence/feed">globalnews.ca</source>
<category>ai</category>
<category>artificial intelligence</category>
<category>globalnews.ca</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>tech</category>
<category>trending</category>
<category>world</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference—far beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward</title>
<link>https://arxiv.org/abs/2602.17558</link>
<guid>arxiv:2602.17558</guid>
<pubDate>Fri, 20 Feb 2026 05:00:00 +0000</pubDate>
<description>arXiv:2602. 17558v1 Announce Type: new Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control.</description>
<source url="https://rss.arxiv.org/rss/cs.CV">arxiv</source>
<category>agents</category>
<category>arxiv</category>
<category>cs.cv</category>
<category>diffusion</category>
<category>llm</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
<category>vision</category>
</item>
<item>
<title>Enterprise AI Automation: A Practical Guide for Large Organizations</title>
<link>https://pub.towardsai.net/enterprise-ai-automation-a-practical-guide-for-large-organizations-7dc7f83f1cc6?source=rss----98111c9905da---4</link>
<guid>title:enterprise ai automation a practical guide for large organizations</guid>
<pubDate>Fri, 20 Feb 2026 04:46:01 +0000</pubDate>
<description>Key Takeaways - Enterprise AI automation is a coordinated system (RPA + ML/LLMs + agents) that manages complex, end-to-end processes, not a single tool but an operating model. - Three layers define the stack traditional automation (hands), intelligent automation (brain), and agentic AI (conductor), each adds adaptability beyond rule-based logic. - The agentic loop — perception → reasoning → action → feedback lets systems adapt when steps fail, making AI effective for the “long tail” of variable, decision-heavy work. - Benefits include 30–50% efficiency gains, faster decisions, on-demand scalability, and measurable cost reduction; early adopters are already in production across finance, healthcare, retail, and manufacturing. - Risks are real Shadow AI, hallucination, prompt injection, and underestimating ongoing operational cost; governance, validation layers, and human-in-the-loop are non-negotiable for production. - Getting started means preparation (alignment, data audit), one high-impact pilot, then scale with governance-as-code and cross-functional teams from day one.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agents</category>
<category>ai</category>
<category>ai-agent</category>
<category>ai-automation</category>
<category>alignment</category>
<category>enterprise-ai</category>
<category>generative-ai</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>reasoning</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Introducing WorldVQA â</title>
<link>https://www.kimi.com/blog/worldvqa.html</link>
<guid>title:introducing worldvqa</guid>
<pubDate>Fri, 20 Feb 2026 08:55:53 +0000</pubDate>
<description>A benchmark for evaluating atomic visual world knowledge in Multimodal LLMs. Authors Kimi Team We are releasing WorldVQA , a new benchmark designed to measure the factual correctness of Multimodal Large Language Models (MLLMs). While recent models have demonstrated impressive capabilities in visual reasoning and description, measuring their reliability regarding visual world knowledge remains a challenge.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>kimi</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Quoting Thariq Shihipar</title>
<link>https://simonwillison.net/2026/Feb/20/thariq-shihipar/#atom-everything</link>
<guid>title:quoting thariq shihipar</guid>
<pubDate>Fri, 20 Feb 2026 07:13:19 +0000</pubDate>
<description>Long running agentic products like Claude Code are made feasible by prompt caching which allows us to reuse computation from previous roundtrips and significantly decrease latency and cost. [...] At Claude Code, we build our entire harness around prompt caching. A high prompt cache hit rate decreases costs and helps us create more generous rate limits for our subscription plans, so we run alerts on our prompt cache hit rate and declare SEVs if they're too low. &amp;mdash; Thariq Shihipar Tags: prompt-engineering , anthropic , claude-code , ai-agents , generative-ai , ai , llms</description>
<source url="https://simonwillison.net/atom/everything/">simonwillison</source>
<category>agents</category>
<category>ai</category>
<category>ai-agents</category>
<category>anthropic</category>
<category>claude-code</category>
<category>engineering</category>
<category>generative-ai</category>
<category>llm</category>
<category>llms</category>
<category>nonpaper</category>
<category>prompt-engineering</category>
<category>serving</category>
<category>simonwillison</category>
<category>tools</category>
</item>
<item>
<title>Why Distributed State Management Is the Context Graph for Physical AI</title>
<link>https://pub.towardsai.net/why-distributed-state-management-is-the-context-graph-for-physical-ai-48e24583170e?source=rss----98111c9905da---4</link>
<guid>title:why distributed state management is the context graph for physical ai</guid>
<pubDate>Fri, 20 Feb 2026 04:41:24 +0000</pubDate>
<description>In my last blog, I wrote about memory as the missing state layer for agents. In this piece, I want to stretch the idea further and look at state across distributed endpoints — local agents, phones, robots, sensors, vehicles — what I think of as the context graph for physical AI, capturing intent, precedent, and decision traces across distributed actors. Not just as a buzzword, but as a world where compute, decisions, and data increasingly live outside centralized clouds. Why distributed state matters, especially now A personal moment in San Francisco made this intuitive for me. One afternoon, I was riding a Waymo through the busy Financial District. Two other Waymos stopped ahead of us, one in the right lane trying to turn left, the other in the left lane trying to turn right.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agents</category>
<category>ai</category>
<category>context-graph</category>
<category>data-infrastructure</category>
<category>distributed-systems</category>
<category>edge-computing</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>physical-ai</category>
<category>pub.towardsai.net</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Why we should expect ruthless sociopath ASI</title>
<link>https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi</link>
<guid>title:why we should expect ruthless sociopath asi</guid>
<pubDate>Wed, 18 Feb 2026 17:28:17 +0000</pubDate>
<description>Published on February 18, 2026 5:28 PM GMT The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i. e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas. ) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs.</description>
<source url="https://www.alignmentforum.org/feed.xml">alignment-forum</source>
<category>agents</category>
<category>alignment</category>
<category>alignment-forum</category>
<category>governance</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Kimi K2.5: Visual Agentic Intelligence â</title>
<link>https://www.kimi.com/blog/kimi-k2-5.html</link>
<guid>title:kimi k2 5 visual agentic intelligence</guid>
<pubDate>Fri, 20 Feb 2026 08:55:54 +0000</pubDate>
<description>Today, we are introducing Kimi K2. 5, the most powerful open-source model to date. Kimi K2. 5 builds on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens. Built as a native multimodal model, K2. 5 delivers state-of-the-art coding and vision capabilities and a self-directed agent swarm paradigm.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>agents</category>
<category>kimi</category>
<category>lab</category>
<category>models</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>product</category>
<category>vision</category>
</item>
<item>
<title>Project Vend: Phase two</title>
<link>https://www.anthropic.com/research/project-vend-2</link>
<guid>title:project vend phase two</guid>
<pubDate>Fri, 20 Feb 2026 08:55:19 +0000</pubDate>
<description>In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend , a free-form experiment exploring how well AIs could do on complex, real-world tasks. Alas, the shopkeeper—a modified version of Claude we named “Claudius”—did not do particularly well. It lost money over time, had a strange identity crisis where it claimed it was a human wearing a blue blazer, and was goaded by mischievous Anthropic employees into selling products (particularly, for some reason, tungsten cubes) at a substantial loss. But the capabilities of large language models in areas like reasoning, writing, coding, and much else besides are increasing at a breathless pace. Has Claudius’s “running a shop” capability shown the same improvement?</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Java Edition of Claude Code CLI is Here! Solon Code CLI Released</title>
<link>https://dev.to/noear/java-edition-of-claude-code-cli-is-here-solon-code-cli-released-1lpb</link>
<guid>title:java edition of claude code cli is here solon code cli released</guid>
<pubDate>Fri, 20 Feb 2026 08:38:35 +0000</pubDate>
<description>When AI meets code, and the essence of Claude Code meets the Solon AI framework—a smart terminal assistant truly belonging to the Java ecosystem is born. Remember the shock when Claude Code CLI was released? That AI assistant capable of chatting in the terminal, generating videos, sending emails, writing code, and understanding project structures finally has a Java implementation! Today, the Solon AI framework officially launches Solon Code CLI — a smart terminal tool that fully embraces the design philosophy of Claude Code CLI while deeply integrating with the Java ecosystem. Compatible with Claude Code Agent Skills specifications Compatible with CLAUDE. md specifications 1.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>claudecode</category>
<category>dev.to</category>
<category>java</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
</item>
<item>
<title>Decision Boundaries, Advanced Planning for Evaluation, AI in Museums, and Building a Secure AI…</title>
<link>https://odsc.medium.com/decision-boundaries-advanced-planning-for-evaluation-ai-in-museums-and-building-a-secure-ai-558f77033985?source=rss-2b9d62538208------2</link>
<guid>title:decision boundaries advanced planning for evaluation ai in museums and building a secure ai</guid>
<pubDate>Thu, 19 Feb 2026 17:01:04 +0000</pubDate>
<description>Decision Boundaries, Advanced Planning for Evaluation, AI in Museums, and Building a Secure AI Assistant Introducing the ODSC AI East AI Engineering Accelerator Starting next week! The ODSC AI Engineering Accelerator is built to help practitioners move from experimentation to real-world AI systems. If your goal is to work on AI systems that ship, scale, and hold up in production, this is where the work begins. Register here for 40% off. AI in Museums: How the Cleveland Museum of Art Built AI-Powered Experiences That Actually Work Explore a real-world case study of AI in museums, featuring over a decade of AI-powered experiences at the Cleveland Museum of Art. Decision Boundaries — From Classical ML to Modern AI Geometry, particularly in the form of decision boundaries, has been central to classical machine learning, even if we didn’t notice, and it is almost surely central to modern AI as well.</description>
<source url="https://medium.com/feed/@odsc">medium.com</source>
<category>agentic-ai</category>
<category>agents</category>
<category>ai</category>
<category>ai-agent</category>
<category>artificial-intelligence</category>
<category>data-science</category>
<category>llm</category>
<category>medium.com</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>vision</category>
</item>
<item>
<title>SWE-bench February 2026 leaderboard update</title>
<link>https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything</link>
<guid>title:swe bench february 2026 leaderboard update</guid>
<pubDate>Thu, 19 Feb 2026 04:48:47 +0000</pubDate>
<description>SWE-bench February 2026 leaderboard update SWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that weren't self-reported by the labs. The fresh results are for their &quot;Bash Only&quot; benchmark, which runs their mini-swe-bench agent (~9,000 lines of Python, here are the prompts they use) against the SWE-bench dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: django/django (850), sympy/sympy (386), scikit-learn/scikit-learn (229), sphinx-doc/sphinx (187), matplotlib/matplotlib (184), pytest-dev/pytest (119), pydata/xarray (110), astropy/astropy (95), pylint-dev/pylint (57), psf/requests (44), mwaskom/seaborn (22), pallets/flask (11). Correction : The Bash only benchmark runs against SWE-bench Verified, not original SWE-bench. Verified is a manually curated subset of 500 samples described here , funded by OpenAI. Here's SWE-bench Verified on Hugging Face - since it's just 2.</description>
<source url="https://simonwillison.net/atom/everything/">simonwillison</source>
<category>agents</category>
<category>ai</category>
<category>ai-in-china</category>
<category>anthropic</category>
<category>benchmarks</category>
<category>claude</category>
<category>coding-agents</category>
<category>django</category>
<category>engineering</category>
<category>generative-ai</category>
<category>llm</category>
<category>llms</category>
<category>minimax</category>
<category>nonpaper</category>
<category>openai</category>
<category>reasoning</category>
<category>rl</category>
<category>simonwillison</category>
<category>tools</category>
</item>
<item>
<title>Why RL Feedback Fails Language Models (And What ERL Fixes)</title>
<link>https://hackernoon.com/why-rl-feedback-fails-language-models-and-what-erl-fixes?source=rss</link>
<guid>title:why rl feedback fails language models and what erl fixes</guid>
<pubDate>Fri, 20 Feb 2026 01:44:59 +0000</pubDate>
<description>ERL adds a reflection step to reinforcement learning: attempt, feedback, explanation, refined attempt. The result: faster learning, higher reward, same inference cost. Read All</description>
<source url="https://hackernoon.com/tagged/ai/feed">hackernoon.com</source>
<category>ai</category>
<category>credit-assignment</category>
<category>erl</category>
<category>experiential-reinforcement</category>
<category>hackernoon.com</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>reinforcement-learning</category>
<category>rl</category>
<category>rlvr-limitations</category>
<category>scalar-reward-signal</category>
<category>serving</category>
<category>sparse-and-delayed-rewards</category>
</item>
<item>
<title>Gemini 3.1 Pro Is Google's Greatest Model Ever! Most Powerful AI EVER! (Fully Tested)</title>
<link>https://www.youtube.com/watch?v=_uQKI-NOCFg</link>
<guid>title:gemini 3 1 pro is google s greatest model ever most powerful ai ever fully tested</guid>
<pubDate>Fri, 20 Feb 2026 00:07:59 +0000</pubDate>
<description>Google is officially back — and Gemini 3. 1 Pro might be the biggest leap we’ve seen in AI this year. 🔗 My Links: Sponsor a Video or Do a Demo of Your Product, Contact me: intheworldzofai@gmail. com 🔥 Become a Patron (Private Discord): https://patreon. com/WorldofAi 🧠 Follow me on Twitter: https://twitter. com/intheworldofai 🚨 Subscribe To The SECOND Channel: https://www.</description>
<source url="https://www.youtube.com/feeds/videos.xml?channel_id=UC2WmuBuFq6gL08QYG-JjXKw">youtube.com</source>
<category>agents</category>
<category>ai</category>
<category>news</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>youtube.com</category>
</item>
<item>
<title>Kimi Introduces Agent Swarm: Let 100 AI Agents Work for You â</title>
<link>https://www.kimi.com/blog/agent-swarm.html</link>
<guid>title:kimi introduces agent swarm let 100 ai agents work for you</guid>
<pubDate>Fri, 20 Feb 2026 08:55:52 +0000</pubDate>
<description>In 2025, if you walked into any AI conference, you may hear the same gospel: faster inference, longer context windows, cheaper inference costs. It's as if we've spent years perfecting the hammer, making it lighter, stronger, more precisely balanced, while never questioning the fact that the carpenter still has only two hands and twenty-four hours in a day. Now, Kimi introduces Agent Swarm. It is not a better hammer. It is a reconstruction of the entire workshop.</description>
<source url="https://www.kimi.com/blog">kimi</source>
<category>agents</category>
<category>kimi</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>product</category>
<category>serving</category>
</item>
<item>
<title>Signs of introspection in large language models</title>
<link>https://www.anthropic.com/research/introspection</link>
<guid>title:signs of introspection in large language models</guid>
<pubDate>Fri, 20 Feb 2026 08:55:24 +0000</pubDate>
<description>Have you ever asked an AI model what’s on its mind? Or to explain how it came up with its responses? Models will sometimes answer questions like these, but it’s hard to know what to make of their answers. Can AI systems really introspect—that is, can they consider their own thoughts? Or do they just make up plausible-sounding answers when they’re asked to do so? Understanding whether AI systems can truly introspect has important implications for their transparency and reliability.</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>research</category>
</item>
<item>
<title>Economic Research</title>
<link>https://www.anthropic.com/research/team/economic-research</link>
<guid>title:economic research</guid>
<pubDate>Fri, 20 Feb 2026 08:55:03 +0000</pubDate>
<description>The Economic Research team studies how AI is reshaping the economy, including work, productivity, and economic opportunity. Through rigorous data collection and analysis, we track AI's real-world economic effects and publish research that helps policymakers, businesses, and the public understand and prepare for the changes ahead. We build the empirical foundation for understanding AI's economic impact. Our flagship Anthropic Economic Index tracks how AI tools are actually being used around the world and across every sector of the economy—moving beyond speculation to measure adoption patterns as they unfold. Alongside our index reports, we produce novel research that studies the implications of AI usage and diffusion—as tracked in the index—for workers, for firms, and for the broader economy. Economic transitions create both opportunity and disruption.</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>anthropic</category>
<category>diffusion</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Alignment</title>
<link>https://www.anthropic.com/research/team/alignment</link>
<guid>title:alignment</guid>
<pubDate>Fri, 20 Feb 2026 08:54:58 +0000</pubDate>
<description>Future AI systems will be even more powerful than today’s, likely in ways that break key assumptions behind current safety techniques. That’s why it’s important to develop sophisticated safeguards to ensure models remain helpful, honest, and harmless. The Alignment team works to understand the challenges ahead and create protocols to train, evaluate, and monitor highly-capable models safely. Alignment researchers validate that models are harmless and honest even under very different circumstances than those under which they were trained. They also develop methods to allow humans to collaborate with language models to verify claims that humans might not be able to on their own. Alignment researchers also systematically look for situations in which models might behave badly, and check whether our existing safeguards are sufficient to deal with risks that human-level capabilities may bring.</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>alignment</category>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
</item>
<item>
<title>Research</title>
<link>https://www.anthropic.com/research</link>
<guid>title:research</guid>
<pubDate>Fri, 20 Feb 2026 08:54:53 +0000</pubDate>
<description>Our research teams investigate the safety, inner workings, and societal impacts of AI models – so that artificial intelligence has a positive impact as it becomes increasingly capable. The mission of the Interpretability team is to discover and understand how large language models work internally, as a foundation for AI safety and positive outcomes. The Alignment team works to understand the risks of AI models and develop ways to ensure that future ones remain helpful, honest, and harmless.</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>alignment</category>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
</item>
<item>
<title>AI Image Detection Software in 2026: Identifying Synthetic and Deepfake Images</title>
<link>https://dev.to/hazel_94/ai-image-detection-software-in-2026-identifying-synthetic-and-deepfake-images-10op</link>
<guid>title:ai image detection software in 2026 identifying synthetic and deepfake images</guid>
<pubDate>Fri, 20 Feb 2026 08:33:59 +0000</pubDate>
<description>As AI image generators and deepfake technologies continue to evolve in 2026, distinguishing authentic visuals from synthetic ones has become significantly more challenging. From educational institutions to media organizations and cybersecurity teams, the need for reliable AI image detection software is now critical. Below is an updated list of leading AI image detection tools in 2026, based on accuracy, transparency, and real-world usability. 1. Winston AI Winston AI has expanded its capabilities beyond text detection and now offers advanced AI image detection features. Key Features: Detects AI-generated and deepfake images Provides confidence scores with visual breakdowns Uses multi-signal analysis instead of single-pattern detection Suitable for educators, publishers, and enterprises Why It Stands Out: Winston AI focuses on layered detection signals, helping reduce false positives while maintaining strong detection accuracy.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>ai</category>
<category>bestaitools</category>
<category>detection</category>
<category>dev.to</category>
<category>diffusion</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>security</category>
</item>
<item>
<title>How I Built a Self-Correcting ML Pipeline That Runs for Days Without Human Intervention</title>
<link>https://dev.to/saman_tabatabaeian/how-i-built-a-self-correcting-ml-pipeline-that-runs-for-days-without-human-intervention-2lm0</link>
<guid>title:how i built a self correcting ml pipeline that runs for days without human intervention</guid>
<pubDate>Fri, 20 Feb 2026 08:32:55 +0000</pubDate>
<description>Most ML pipelines are batch jobs: data in, predictions out, human reviews results. But what if you need a pipeline that runs autonomously for days, continuously ingesting data from external sources, making decisions, and — critically — correcting its own mistakes without anyone watching? That's what I built for AstroLens , an open-source tool for detecting astronomical anomalies in sky survey images. Version 1. 1. 0 introduces Streaming Discovery : a mode where the system runs for days, downloads tens of thousands of images from sky surveys, analyzes each one with an ML ensemble, and generates daily reports — all while continuously adjusting its own parameters.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>ai</category>
<category>astronomy</category>
<category>dev.to</category>
<category>machinelearning</category>
<category>news</category>
<category>nonpaper</category>
<category>opensource</category>
<category>python</category>
<category>rl</category>
<category>vision</category>
</item>
</channel>
</rss>