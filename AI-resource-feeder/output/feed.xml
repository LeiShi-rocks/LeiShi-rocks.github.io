<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Trend Feed</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers/blogs ranked by recency + popularity + relevance</description>
<lastBuildDate>Sat, 07 Feb 2026 18:44:53 +0000</lastBuildDate>
<item>
<title>Anthropic partners with Allen Institute and Howard Hughes Medical Institute to accelerate scientific discovery</title>
<link>https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute</link>
<guid>title:anthropic partners with allen institute and howard hughes medical institute to accelerate scientific discovery</guid>
<pubDate>Sat, 07 Feb 2026 18:44:48 +0000</pubDate>
<description>Modern biological research generates data at unprecedented scale—from single-cell sequencing to whole-brain connectomics—yet transforming that data into validated biological insights remains a fundamental bottleneck. Knowledge synthesis, hypothesis generation, and experimental interpretation still depend on manual processes that can't keep pace with the data being produced. Today, Anthropic is announcing two flagship partnerships designed to close that gap. The Allen Institute and Howard Hughes Medical Institute (HHMI) will serve as founding partners in life sciences, extending Claude’s capabilities to frontier scientific research and enabling teams of scientists to work more effectively together and take on ambitious scientific challenges. Each collaboration brings together Anthropic's expertise in foundation models, agentic systems, and interpretability with world-class research institutions tackling distinct but complementary problems in biology and biomedical science. These partnerships position Claude at the center of scientific experimentation and will build a foundation in which scientists actively use Claude to plan and execute experiments.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Apple’s Xcode now supports the Claude Agent SDK</title>
<link>https://www.anthropic.com/news/apple-xcode-claude-agent-sdk</link>
<guid>title:apple s xcode now supports the claude agent sdk</guid>
<pubDate>Sat, 07 Feb 2026 18:44:48 +0000</pubDate>
<description>Apple's Xcode is where developers build, test, and distribute apps for Apple platforms, including iPhone, iPad, Mac, Apple Watch, Apple Vision Pro, and Apple TV. In September, we announced that developers would have access to Claude Sonnet 4 in Xcode 26. Claude could be used to write code, debug, and generate documentation—but it was limited to helping with individual, turn-by-turn requests. Now, Xcode 26.3 introduces a native integration with the Claude Agent SDK , the same underlying harness that powers Claude Code. Developers get the full power of Claude Code directly in Xcode—including subagents, background tasks, and plugins—all without leaving the IDE.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Introducing Claude Sonnet 4.5</title>
<link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
<guid>title:introducing claude sonnet 4 5</guid>
<pubDate>Sat, 07 Feb 2026 18:44:48 +0000</pubDate>
<description>Claude Sonnet 4. 5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Code is everywhere.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</title>
<link>https://tldr.takara.ai/p/2602.05827</link>
<guid>title:sparse video generation propels real world beyond the view vision language navigation</guid>
<pubDate>Thu, 05 Feb 2026 16:16:13 +0000</pubDate>
<description>Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>ServiceNow chooses Claude to power customer apps and increase internal productivity</title>
<link>https://www.anthropic.com/news/servicenow-anthropic-claude</link>
<guid>title:servicenow chooses claude to power customer apps and increase internal productivity</guid>
<pubDate>Sat, 07 Feb 2026 18:44:49 +0000</pubDate>
<description>As enterprises move beyond experimenting with AI and start putting it into production across their core business operations, scale and security matters just as much as capabilities. With this in mind, ServiceNow, which helps large companies manage and automate everything from IT support to HR to customer service on a single platform, has chosen Claude as its default model for its ServiceNow Build Agent, and as a preferred model across the ServiceNow AI Platform. Build Agent helps developers of all skill levels build and use Claude as an “agent” that can reason, decide on actions, and execute tasks autonomously. In addition, ServiceNow is rolling out Claude and Claude Code across its global workforce of more than 29,000 employees to streamline sales preparation—cutting seller preparation time by 95% and boosting engineering productivity with Claude Code to reduce the time between idea and implementation across the organization.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Sat, 07 Feb 2026 18:44:47 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Sat, 07 Feb 2026 18:44:47 +0000</pubDate>
<description>We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. The first AI-assisted drive on another planet.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>RISE-Video: Can Video Generators Decode Implicit World Rules?</title>
<link>https://tldr.takara.ai/p/2602.05986</link>
<guid>title:rise video can video generators decode implicit world rules</guid>
<pubDate>Thu, 05 Feb 2026 18:36:10 +0000</pubDate>
<description>While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Introducing Claude Haiku 4.5</title>
<link>https://www.anthropic.com/news/claude-haiku-4-5</link>
<guid>title:introducing claude haiku 4 5</guid>
<pubDate>Sat, 07 Feb 2026 18:44:48 +0000</pubDate>
<description>Claude Haiku 4. 5, our latest small model, is available today to all users. What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4. 5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Sat, 07 Feb 2026 18:44:47 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>DFlash: Block Diffusion for Flash Speculative Decoding</title>
<link>https://tldr.takara.ai/p/2602.06036</link>
<guid>title:dflash block diffusion for flash speculative decoding</guid>
<pubDate>Thu, 05 Feb 2026 18:59:30 +0000</pubDate>
<description>Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</title>
<link>https://tldr.takara.ai/p/2602.06035</link>
<guid>title:interprior scaling generative control for physics based human object interactions</guid>
<pubDate>Thu, 05 Feb 2026 18:59:27 +0000</pubDate>
<description>Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>
<link>https://tldr.takara.ai/p/2602.05951</link>
<guid>title:better source better flow learning condition dependent source distribution for flow matching</guid>
<pubDate>Thu, 05 Feb 2026 18:08:20 +0000</pubDate>
<description>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Regularized Calibration with Successive Rounding for Post-Training Quantization</title>
<link>https://tldr.takara.ai/p/2602.05902</link>
<guid>title:regularized calibration with successive rounding for post training quantization</guid>
<pubDate>Thu, 05 Feb 2026 17:18:02 +0000</pubDate>
<description>Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>efficiency</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
<link>https://tldr.takara.ai/p/2602.05859</link>
<guid>title:dlm scope mechanistic interpretability of diffusion language models via sparse autoencoders</guid>
<pubDate>Thu, 05 Feb 2026 16:41:25 +0000</pubDate>
<description>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</title>
<link>https://tldr.takara.ai/p/2602.05633</link>
<guid>title:castle a comprehensive benchmark for evaluating student tailored personalized safety in large language models</guid>
<pubDate>Thu, 05 Feb 2026 13:13:19 +0000</pubDate>
<description>Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Multi-Task GRPO: Reliable LLM Reasoning Across Tasks</title>
<link>https://tldr.takara.ai/p/2602.05547</link>
<guid>title:multi task grpo reliable llm reasoning across tasks</guid>
<pubDate>Thu, 05 Feb 2026 11:06:37 +0000</pubDate>
<description>RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Natively Adaptive Interfaces: A new framework for AI accessibility</title>
<link>https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/</link>
<guid>title:natively adaptive interfaces a new framework for ai accessibility</guid>
<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>accessibility</category>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference—far beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>The CEO of private jet firm Flexjet explains how it prepares for the Super Bowl — one of its busiest and most expensive times of the year</title>
<link>https://www.businessinsider.com/super-bowl-private-jets-flexjet-ceo-explains-how-it-prepares-2026-2</link>
<guid>title:the ceo of private jet firm flexjet explains how it prepares for the super bowl one of its busiest and most expensive times of the year</guid>
<pubDate>Sat, 07 Feb 2026 10:57:01 +0000</pubDate>
<description>The Super Bowl will see private jets charged event fees in the tens of thousands, and special procedures to deal with high traffic.</description>
<source url="https://feeds.businessinsider.com/custom/all">feeds.businessinsider.com</source>
<category>ai</category>
<category>aviation</category>
<category>bay-area</category>
<category>ceos</category>
<category>feeds.businessinsider.com</category>
<category>flexjet</category>
<category>new-england-patriots</category>
<category>news</category>
<category>nonpaper</category>
<category>private-jets</category>
<category>seattle-seahawks</category>
<category>super-bowl</category>
<category>transportation</category>
<category>trending-uk</category>
</item>
<item>
<title>How Google Cloud is helping Team USA elevate their tricks with AI</title>
<link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/</link>
<guid>title:how google cloud is helping team usa elevate their tricks with ai</guid>
<pubDate>Thu, 05 Feb 2026 16:00:00 +0000</pubDate>
<description>A woman outdoors in the snow looks at a tablet. A half pipe is behind her.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google cloud</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Watch our new Gemini ad ahead of football’s biggest weekend</title>
<link>https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/</link>
<guid>title:watch our new gemini ad ahead of football s biggest weekend</guid>
<pubDate>Thu, 05 Feb 2026 14:30:00 +0000</pubDate>
<description>A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial’</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Build with Kimi K2.5 Multimodal VLM Using NVIDIA GPU-Accelerated Endpoints</title>
<link>https://developer.nvidia.com/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/</link>
<guid>title:build with kimi k2 5 multimodal vlm using nvidia gpu accelerated endpoints</guid>
<pubDate>Wed, 04 Feb 2026 19:46:33 +0000</pubDate>
<description>Kimi K2.5 is the newest open vision language model (VLM) from the Kimi family of models. Kimi K2.5 is a general-purpose multimodal model that excels in current...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai agent</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>top stories</category>
<category>training</category>
<category>vision</category>
<category>vlms</category>
</item>
<item>
<title>How AI tools can redefine universal design to increase accessibility</title>
<link>https://research.google/blog/how-ai-agents-can-redefine-universal-design-to-increase-accessibility/</link>
<guid>title:how ai tools can redefine universal design to increase accessibility</guid>
<pubDate>Thu, 05 Feb 2026 08:28:00 +0000</pubDate>
<description>Education Innovation</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>education innovation</category>
<category>google-research</category>
<category>machine intelligence</category>
<category>models</category>
<category>natural language processing</category>
<category>nonpaper</category>
<category>research</category>
<category>responsible ai</category>
<category>science</category>
</item>
<item>
<title>The latest AI news we announced in January</title>
<link>https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/</link>
<guid>title:the latest ai news we announced in january</guid>
<pubDate>Wed, 04 Feb 2026 16:55:00 +0000</pubDate>
<description>mp4 showing a carousel of images including a card reading &quot;Help that's made for you&quot;</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>arts &amp; culture</category>
<category>chrome</category>
<category>developer tools</category>
<category>gemini</category>
<category>gemini app</category>
<category>gmail</category>
<category>google</category>
<category>google ads</category>
<category>google cloud</category>
<category>google deepmind</category>
<category>lab</category>
<category>learning &amp; education</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>search</category>
<category>shopping</category>
</item>
<item>
<title>​Sequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
<link>https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/</link>
<guid>title:sequential attention making ai models leaner and faster without sacrificing accuracy</guid>
<pubDate>Wed, 04 Feb 2026 15:14:00 +0000</pubDate>
<description>Algorithms &amp; Theory</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>algorithms &amp; theory</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>Collaborating on a nationwide randomized study of AI in real-world virtual care</title>
<link>https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care/</link>
<guid>title:collaborating on a nationwide randomized study of ai in real world virtual care</guid>
<pubDate>Tue, 03 Feb 2026 18:15:01 +0000</pubDate>
<description>Generative AI</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>generative ai</category>
<category>google-research</category>
<category>health &amp; bioscience</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>rl</category>
<category>science</category>
</item>
<item>
<title>How to Build License-Compliant Synthetic Data Pipelines for AI Model Distillation</title>
<link>https://developer.nvidia.com/blog/how-to-build-license-compliant-synthetic-data-pipelines-for-ai-model-distillation/</link>
<guid>title:how to build license compliant synthetic data pipelines for ai model distillation</guid>
<pubDate>Thu, 05 Feb 2026 18:00:00 +0000</pubDate>
<description>Specialized AI models are built to perform specific tasks or solve particular problems. But if you’ve ever tried to fine-tune or distill a domain-specific...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llms</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>pandas</category>
<category>synthetic data generation</category>
<category>training</category>
<category>training ai models</category>
</item>
<item>
<title>How Painkiller RTX Uses Generative AI to Modernize Game Assets at Scale</title>
<link>https://developer.nvidia.com/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/</link>
<guid>title:how painkiller rtx uses generative ai to modernize game assets at scale</guid>
<pubDate>Thu, 05 Feb 2026 14:00:00 +0000</pubDate>
<description>Painkiller RTX sets a new standard for how small teams can balance massive visual ambition with limited resources by integrating generative AI. By upscaling...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>content creation / rendering</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>news</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title>
<link>https://arxiv.org/abs/2602.05472</link>
<guid>arxiv:2602.05472</guid>
<pubDate>Sat, 07 Feb 2026 05:00:00 +0000</pubDate>
<description>arXiv:2602. 05472v1 Announce Type: new Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty.</description>
<source url="https://rss.arxiv.org/rss/cs.AI">arxiv</source>
<category>ai</category>
<category>alignment</category>
<category>arxiv</category>
<category>cs.ai</category>
<category>llm</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
<category>serving</category>
<category>vision</category>
</item>
</channel>
</rss>