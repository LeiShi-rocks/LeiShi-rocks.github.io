<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Trend Feed</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI papers/blogs ranked by recency + popularity + relevance</description>
<lastBuildDate>Sat, 07 Feb 2026 09:04:14 +0000</lastBuildDate>
<item>
<title>Claude Code #4: From The Before Times</title>
<link>https://www.lesswrong.com/posts/iwX2aJPKtyKAbLdip/claude-code-4-from-the-before-times</link>
<guid>title:claude code 4 from the before times</guid>
<pubDate>Fri, 06 Feb 2026 18:01:08 +0000</pubDate>
<description>Published on February 6, 2026 6:01 PM GMT Claude Opus 4. 6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5. 3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Anthropic partners with Allen Institute and Howard Hughes Medical Institute to accelerate scientific discovery</title>
<link>https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute</link>
<guid>title:anthropic partners with allen institute and howard hughes medical institute to accelerate scientific discovery</guid>
<pubDate>Sat, 07 Feb 2026 09:04:09 +0000</pubDate>
<description>Modern biological research generates data at unprecedented scale—from single-cell sequencing to whole-brain connectomics—yet transforming that data into validated biological insights remains a fundamental bottleneck. Knowledge synthesis, hypothesis generation, and experimental interpretation still depend on manual processes that can't keep pace with the data being produced. Today, Anthropic is announcing two flagship partnerships designed to close that gap. The Allen Institute and Howard Hughes Medical Institute (HHMI) will serve as founding partners in life sciences, extending Claude’s capabilities to frontier scientific research and enabling teams of scientists to work more effectively together and take on ambitious scientific challenges. Each collaboration brings together Anthropic's expertise in foundation models, agentic systems, and interpretability with world-class research institutions tackling distinct but complementary problems in biology and biomedical science. These partnerships position Claude at the center of scientific experimentation and will build a foundation in which scientists actively use Claude to plan and execute experiments.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Apple’s Xcode now supports the Claude Agent SDK</title>
<link>https://www.anthropic.com/news/apple-xcode-claude-agent-sdk</link>
<guid>title:apple s xcode now supports the claude agent sdk</guid>
<pubDate>Sat, 07 Feb 2026 09:04:08 +0000</pubDate>
<description>Apple's Xcode is where developers build, test, and distribute apps for Apple platforms, including iPhone, iPad, Mac, Apple Watch, Apple Vision Pro, and Apple TV. In September, we announced that developers would have access to Claude Sonnet 4 in Xcode 26. Claude could be used to write code, debug, and generate documentation—but it was limited to helping with individual, turn-by-turn requests. Now, Xcode 26.3 introduces a native integration with the Claude Agent SDK , the same underlying harness that powers Claude Code. Developers get the full power of Claude Code directly in Xcode—including subagents, background tasks, and plugins—all without leaving the IDE.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Introducing Claude Sonnet 4.5</title>
<link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
<guid>title:introducing claude sonnet 4 5</guid>
<pubDate>Sat, 07 Feb 2026 09:04:07 +0000</pubDate>
<description>Claude Sonnet 4. 5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Code is everywhere.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>In (highly contingent!) defense of interpretability-in-the-loop ML training</title>
<link>https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop</link>
<guid>title:in highly contingent defense of interpretability in the loop ml training</guid>
<pubDate>Fri, 06 Feb 2026 16:32:27 +0000</pubDate>
<description>Published on February 6, 2026 4:32 PM GMT Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function. Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&amp;nbsp; Yudkowsky 2022 : When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect. &amp;nbsp; Optimizing against an interpreted thought optimizes against interpretability. Or&amp;nbsp; Zvi 2025 : The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M].</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>world-models</category>
</item>
<item>
<title>Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</title>
<link>https://tldr.takara.ai/p/2602.05827</link>
<guid>title:sparse video generation propels real world beyond the view vision language navigation</guid>
<pubDate>Thu, 05 Feb 2026 16:16:13 +0000</pubDate>
<description>Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>agents</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title>
<link>https://www.lesswrong.com/posts/dTfpSfTfYs7qg4MFi/data-centric-interpretability-for-llm-based-multi-agent</link>
<guid>title:data centric interpretability for llm based multi agent reinforcement learning</guid>
<pubDate>Fri, 06 Feb 2026 19:27:09 +0000</pubDate>
<description>Published on February 6, 2026 7:27 PM GMT TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputs paper Abstract Large language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14. 2%.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>RISE-Video: Can Video Generators Decode Implicit World Rules?</title>
<link>https://tldr.takara.ai/p/2602.05986</link>
<guid>title:rise video can video generators decode implicit world rules</guid>
<pubDate>Thu, 05 Feb 2026 18:36:10 +0000</pubDate>
<description>While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Spectral Signatures of Gradual Disempowerment</title>
<link>https://www.lesswrong.com/posts/erTAgDriWAw3evecP/spectral-signatures-of-gradual-disempowerment</link>
<guid>title:spectral signatures of gradual disempowerment</guid>
<pubDate>Fri, 06 Feb 2026 15:08:08 +0000</pubDate>
<description>Published on February 6, 2026 3:08 PM GMT TL;DR AI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance. Introduction AI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases. The stubborn part is that it operates across institutional boundaries simultaneously.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>ServiceNow chooses Claude to power customer apps and increase internal productivity</title>
<link>https://www.anthropic.com/news/servicenow-anthropic-claude</link>
<guid>title:servicenow chooses claude to power customer apps and increase internal productivity</guid>
<pubDate>Sat, 07 Feb 2026 09:04:09 +0000</pubDate>
<description>As enterprises move beyond experimenting with AI and start putting it into production across their core business operations, scale and security matters just as much as capabilities. With this in mind, ServiceNow, which helps large companies manage and automate everything from IT support to HR to customer service on a single platform, has chosen Claude as its default model for its ServiceNow Build Agent, and as a preferred model across the ServiceNow AI Platform. Build Agent helps developers of all skill levels build and use Claude as an “agent” that can reason, decide on actions, and execute tasks autonomously. In addition, ServiceNow is rolling out Claude and Claude Code across its global workforce of more than 29,000 employees to streamline sales preparation—cutting seller preparation time by 95% and boosting engineering productivity with Claude Code to reduce the time between idea and implementation across the organization.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Sat, 07 Feb 2026 09:04:06 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Sat, 07 Feb 2026 09:04:05 +0000</pubDate>
<description>We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. The first AI-assisted drive on another planet.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>DFlash: Block Diffusion for Flash Speculative Decoding</title>
<link>https://tldr.takara.ai/p/2602.06036</link>
<guid>title:dflash block diffusion for flash speculative decoding</guid>
<pubDate>Thu, 05 Feb 2026 18:59:30 +0000</pubDate>
<description>Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</title>
<link>https://tldr.takara.ai/p/2602.06035</link>
<guid>title:interprior scaling generative control for physics based human object interactions</guid>
<pubDate>Thu, 05 Feb 2026 18:59:27 +0000</pubDate>
<description>Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>ml</category>
<category>multimodal</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
<category>robotics</category>
</item>
<item>
<title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>
<link>https://tldr.takara.ai/p/2602.05951</link>
<guid>title:better source better flow learning condition dependent source distribution for flow matching</guid>
<pubDate>Thu, 05 Feb 2026 18:08:20 +0000</pubDate>
<description>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Regularized Calibration with Successive Rounding for Post-Training Quantization</title>
<link>https://tldr.takara.ai/p/2602.05902</link>
<guid>title:regularized calibration with successive rounding for post training quantization</guid>
<pubDate>Thu, 05 Feb 2026 17:18:02 +0000</pubDate>
<description>Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>efficiency</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>serving</category>
</item>
<item>
<title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
<link>https://tldr.takara.ai/p/2602.05859</link>
<guid>title:dlm scope mechanistic interpretability of diffusion language models via sparse autoencoders</guid>
<pubDate>Thu, 05 Feb 2026 16:41:25 +0000</pubDate>
<description>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>diffusion</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</title>
<link>https://tldr.takara.ai/p/2602.05633</link>
<guid>title:castle a comprehensive benchmark for evaluating student tailored personalized safety in large language models</guid>
<pubDate>Thu, 05 Feb 2026 13:13:19 +0000</pubDate>
<description>Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>alignment</category>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>rl</category>
</item>
<item>
<title>Multi-Task GRPO: Reliable LLM Reasoning Across Tasks</title>
<link>https://tldr.takara.ai/p/2602.05547</link>
<guid>title:multi task grpo reliable llm reasoning across tasks</guid>
<pubDate>Thu, 05 Feb 2026 11:06:37 +0000</pubDate>
<description>RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy.</description>
<source url="https://papers.takara.ai/api/feed">huggingface</source>
<category>daily</category>
<category>huggingface</category>
<category>llm</category>
<category>ml</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>rl</category>
</item>
<item>
<title>Introducing Claude Haiku 4.5</title>
<link>https://www.anthropic.com/news/claude-haiku-4-5</link>
<guid>title:introducing claude haiku 4 5</guid>
<pubDate>Sat, 07 Feb 2026 09:04:08 +0000</pubDate>
<description>Claude Haiku 4. 5, our latest small model, is available today to all users. What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4. 5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Sat, 07 Feb 2026 09:04:07 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Robust Finite Policies are Nontrivially Structured</title>
<link>https://www.lesswrong.com/posts/ieX8nK2b2i4JDRH5s/robust-finite-policies-are-nontrivially-structured</link>
<guid>title:robust finite policies are nontrivially structured</guid>
<pubDate>Fri, 06 Feb 2026 17:52:22 +0000</pubDate>
<description>Published on February 6, 2026 5:47 PM GMT This post was created during the Dovetail Research Fellowship. Thanks to Alex , Alfred , &amp;nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions. Overview The proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem , which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone. For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature. We begin by defining every part of the framework.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>world-models</category>
</item>
<item>
<title>Playing with an Infrared Camera</title>
<link>https://www.lesswrong.com/posts/ALSFkR23qjWjctyhg/playing-with-an-infrared-camera</link>
<guid>title:playing with an infrared camera</guid>
<pubDate>Sat, 07 Feb 2026 03:30:46 +0000</pubDate>
<description>Published on February 7, 2026 3:30 AM GMT I recently got a Thermal Master P1 infrared camera attachment for my phone. The goal was a house project, but it's also a great toy, especially with the kids. Getting a room pitch black but still being able to 'see' with the phone was fun for a bit. The real fun, though, was in exploring to observe all these thermal properties we'd never thought about. Here's my selfie: Light is warmer, dark is cooler. My glasses aren't cool, they're just IR-opaque.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Honey, I shrunk the brain</title>
<link>https://www.lesswrong.com/posts/KvbBYaKmGcJKvvWd8/honey-i-shrunk-the-brain</link>
<guid>doi:10.3389/fmedt.2024.1400615/full</guid>
<pubDate>Sat, 07 Feb 2026 00:01:47 +0000</pubDate>
<description>Published on February 7, 2026 12:01 AM GMT When cryoprotectants are perfused through the blood vessels in the brain, they cannot cross the blood-brain barrier as fast as water can move in the opposite direction. And cryoprotectants generally have a much higher osmotic concentration than the typical blood plasma. For example, the cryoprotectant solution M22 has an osmotic concentration around 100 times higher. As a result, in a successful cryoprotectant perfusion (without fixatives), water rushes out of the tissue into the blood vessels, the tissue dehydrates, and you end up with a shrunken brain that is visibly pulled away from the skull. The brain weight goes down by 50% or more. This is currently considered a good sign of cryoprotectant perfusion quality.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Strategy of von Neumann and strategy of Rosenbergs</title>
<link>https://www.lesswrong.com/posts/orJPh4QdCicDt7c3E/strategy-of-von-neumann-and-strategy-of-rosenbergs</link>
<guid>title:strategy of von neumann and strategy of rosenbergs</guid>
<pubDate>Fri, 06 Feb 2026 22:50:12 +0000</pubDate>
<description>Published on February 6, 2026 10:50 PM GMT This is not a call for espionage, but an analysis of another strategy Von Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project). My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Parks Aren't Nature</title>
<link>https://www.lesswrong.com/posts/cjxPFxAe5WRKA6SeF/parks-aren-t-nature</link>
<guid>title:parks aren t nature</guid>
<pubDate>Fri, 06 Feb 2026 18:27:05 +0000</pubDate>
<description>Published on February 6, 2026 6:27 PM GMT I. I love dogs. I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy! ”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this. Why do I love dogs?</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Natively Adaptive Interfaces: A new framework for AI accessibility</title>
<link>https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/</link>
<guid>title:natively adaptive interfaces a new framework for ai accessibility</guid>
<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>accessibility</category>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference—far beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>How Google Cloud is helping Team USA elevate their tricks with AI</title>
<link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/</link>
<guid>title:how google cloud is helping team usa elevate their tricks with ai</guid>
<pubDate>Thu, 05 Feb 2026 16:00:00 +0000</pubDate>
<description>A woman outdoors in the snow looks at a tablet. A half pipe is behind her.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google cloud</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title>
<link>https://arxiv.org/abs/2602.05472</link>
<guid>arxiv:2602.05472</guid>
<pubDate>Sat, 07 Feb 2026 05:00:00 +0000</pubDate>
<description>arXiv:2602. 05472v1 Announce Type: new Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty.</description>
<source url="https://rss.arxiv.org/rss/cs.AI">arxiv</source>
<category>ai</category>
<category>alignment</category>
<category>arxiv</category>
<category>cs.ai</category>
<category>llm</category>
<category>paper</category>
<category>papers</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
<category>serving</category>
<category>vision</category>
</item>
</channel>
</rss>