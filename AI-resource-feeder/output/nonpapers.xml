<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI News &amp; Posts</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI labs, blogs, and project updates</description>
<lastBuildDate>Sat, 07 Feb 2026 08:40:36 +0000</lastBuildDate>
<item>
<title>Claude Code #4: From The Before Times</title>
<link>https://www.lesswrong.com/posts/iwX2aJPKtyKAbLdip/claude-code-4-from-the-before-times</link>
<guid>title:claude code 4 from the before times</guid>
<pubDate>Fri, 06 Feb 2026 18:01:08 +0000</pubDate>
<description>Published on February 6, 2026 6:01 PM GMT Claude Opus 4. 6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5. 3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Anthropic partners with Allen Institute and Howard Hughes Medical Institute to accelerate scientific discovery</title>
<link>https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute</link>
<guid>title:anthropic partners with allen institute and howard hughes medical institute to accelerate scientific discovery</guid>
<pubDate>Sat, 07 Feb 2026 08:40:32 +0000</pubDate>
<description>Modern biological research generates data at unprecedented scale—from single-cell sequencing to whole-brain connectomics—yet transforming that data into validated biological insights remains a fundamental bottleneck. Knowledge synthesis, hypothesis generation, and experimental interpretation still depend on manual processes that can't keep pace with the data being produced. Today, Anthropic is announcing two flagship partnerships designed to close that gap. The Allen Institute and Howard Hughes Medical Institute (HHMI) will serve as founding partners in life sciences, extending Claude’s capabilities to frontier scientific research and enabling teams of scientists to work more effectively together and take on ambitious scientific challenges. Each collaboration brings together Anthropic's expertise in foundation models, agentic systems, and interpretability with world-class research institutions tackling distinct but complementary problems in biology and biomedical science. These partnerships position Claude at the center of scientific experimentation and will build a foundation in which scientists actively use Claude to plan and execute experiments.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Apple’s Xcode now supports the Claude Agent SDK</title>
<link>https://www.anthropic.com/news/apple-xcode-claude-agent-sdk</link>
<guid>title:apple s xcode now supports the claude agent sdk</guid>
<pubDate>Sat, 07 Feb 2026 08:40:32 +0000</pubDate>
<description>Apple's Xcode is where developers build, test, and distribute apps for Apple platforms, including iPhone, iPad, Mac, Apple Watch, Apple Vision Pro, and Apple TV. In September, we announced that developers would have access to Claude Sonnet 4 in Xcode 26. Claude could be used to write code, debug, and generate documentation—but it was limited to helping with individual, turn-by-turn requests. Now, Xcode 26.3 introduces a native integration with the Claude Agent SDK , the same underlying harness that powers Claude Code. Developers get the full power of Claude Code directly in Xcode—including subagents, background tasks, and plugins—all without leaving the IDE.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Introducing Claude Sonnet 4.5</title>
<link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
<guid>title:introducing claude sonnet 4 5</guid>
<pubDate>Sat, 07 Feb 2026 08:40:31 +0000</pubDate>
<description>Claude Sonnet 4. 5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Code is everywhere.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>In (highly contingent!) defense of interpretability-in-the-loop ML training</title>
<link>https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop</link>
<guid>title:in highly contingent defense of interpretability in the loop ml training</guid>
<pubDate>Fri, 06 Feb 2026 16:32:27 +0000</pubDate>
<description>Published on February 6, 2026 4:32 PM GMT Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function. Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&amp;nbsp; Yudkowsky 2022 : When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect. &amp;nbsp; Optimizing against an interpreted thought optimizes against interpretability. Or&amp;nbsp; Zvi 2025 : The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M].</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>world-models</category>
</item>
<item>
<title>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title>
<link>https://www.lesswrong.com/posts/dTfpSfTfYs7qg4MFi/data-centric-interpretability-for-llm-based-multi-agent</link>
<guid>title:data centric interpretability for llm based multi agent reinforcement learning</guid>
<pubDate>Fri, 06 Feb 2026 19:27:09 +0000</pubDate>
<description>Published on February 6, 2026 7:27 PM GMT TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputs paper Abstract Large language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14. 2%.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Spectral Signatures of Gradual Disempowerment</title>
<link>https://www.lesswrong.com/posts/erTAgDriWAw3evecP/spectral-signatures-of-gradual-disempowerment</link>
<guid>title:spectral signatures of gradual disempowerment</guid>
<pubDate>Fri, 06 Feb 2026 15:08:08 +0000</pubDate>
<description>Published on February 6, 2026 3:08 PM GMT TL;DR AI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance. Introduction AI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases. The stubborn part is that it operates across institutional boundaries simultaneously.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>ServiceNow chooses Claude to power customer apps and increase internal productivity</title>
<link>https://www.anthropic.com/news/servicenow-anthropic-claude</link>
<guid>title:servicenow chooses claude to power customer apps and increase internal productivity</guid>
<pubDate>Sat, 07 Feb 2026 08:40:33 +0000</pubDate>
<description>As enterprises move beyond experimenting with AI and start putting it into production across their core business operations, scale and security matters just as much as capabilities. With this in mind, ServiceNow, which helps large companies manage and automate everything from IT support to HR to customer service on a single platform, has chosen Claude as its default model for its ServiceNow Build Agent, and as a preferred model across the ServiceNow AI Platform. Build Agent helps developers of all skill levels build and use Claude as an “agent” that can reason, decide on actions, and execute tasks autonomously. In addition, ServiceNow is rolling out Claude and Claude Code across its global workforce of more than 29,000 employees to streamline sales preparation—cutting seller preparation time by 95% and boosting engineering productivity with Claude Code to reduce the time between idea and implementation across the organization.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Sat, 07 Feb 2026 08:40:30 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Sat, 07 Feb 2026 08:40:28 +0000</pubDate>
<description>We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. The first AI-assisted drive on another planet.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing GPT-5.3-Codex</title>
<link>https://openai.com/index/introducing-gpt-5-3-codex</link>
<guid>title:introducing gpt 5 3 codex</guid>
<pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
<description>GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.</description>
<source url="https://openai.com/news/rss.xml">openai</source>
<category>agents</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>openai</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Haiku 4.5</title>
<link>https://www.anthropic.com/news/claude-haiku-4-5</link>
<guid>title:introducing claude haiku 4 5</guid>
<pubDate>Sat, 07 Feb 2026 08:40:32 +0000</pubDate>
<description>Claude Haiku 4. 5, our latest small model, is available today to all users. What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4. 5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Sat, 07 Feb 2026 08:40:30 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Making AI work for everyone, everywhere: our approach to localization</title>
<link>https://openai.com/index/our-approach-to-localization</link>
<guid>title:making ai work for everyone everywhere our approach to localization</guid>
<pubDate>Fri, 06 Feb 2026 10:00:00 +0000</pubDate>
<description>OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.</description>
<source url="https://openai.com/news/rss.xml">openai</source>
<category>global affairs</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>openai</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Korea privacy policy</title>
<link>https://openai.com/policies/kr-privacy-policy</link>
<guid>title:korea privacy policy</guid>
<pubDate>Fri, 06 Feb 2026 10:00:00 +0000</pubDate>
<description>Korea privacy policy</description>
<source url="https://openai.com/news/rss.xml">openai</source>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>openai</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Robust Finite Policies are Nontrivially Structured</title>
<link>https://www.lesswrong.com/posts/ieX8nK2b2i4JDRH5s/robust-finite-policies-are-nontrivially-structured</link>
<guid>title:robust finite policies are nontrivially structured</guid>
<pubDate>Fri, 06 Feb 2026 17:52:22 +0000</pubDate>
<description>Published on February 6, 2026 5:47 PM GMT This post was created during the Dovetail Research Fellowship. Thanks to Alex , Alfred , &amp;nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions. Overview The proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem , which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone. For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature. We begin by defining every part of the framework.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>world-models</category>
</item>
<item>
<title>Voting Results for the 2024 Review</title>
<link>https://www.lesswrong.com/posts/uk48L6j28iiAyvPKJ/voting-results-for-the-2024-review</link>
<guid>title:voting results for the 2024 review</guid>
<pubDate>Sat, 07 Feb 2026 03:48:26 +0000</pubDate>
<description>Published on February 7, 2026 3:48 AM GMT The votes are in for the 2024 Review! 4,826 posts were written in 2024. 671 of them were nominated. 196 of them got at least one review, and a positive review-vote total. 50 of them shall be displayed in the Best of LessWrong, Year 2024. Reviews 94 people wrote reviews.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>GPT-5.3-Codex System Card</title>
<link>https://openai.com/index/gpt-5-3-codex-system-card</link>
<guid>title:gpt 5 3 codex system card</guid>
<pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
<description>GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.</description>
<source url="https://openai.com/news/rss.xml">openai</source>
<category>agents</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>openai</category>
<category>product</category>
<category>publication</category>
<category>reasoning</category>
<category>safety</category>
</item>
<item>
<title>Introducing OpenAI Frontier</title>
<link>https://openai.com/index/introducing-openai-frontier</link>
<guid>title:introducing openai frontier</guid>
<pubDate>Thu, 05 Feb 2026 06:00:00 +0000</pubDate>
<description>OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.</description>
<source url="https://openai.com/news/rss.xml">openai</source>
<category>agents</category>
<category>lab</category>
<category>models</category>
<category>nonpaper</category>
<category>openai</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Playing with an Infrared Camera</title>
<link>https://www.lesswrong.com/posts/ALSFkR23qjWjctyhg/playing-with-an-infrared-camera</link>
<guid>title:playing with an infrared camera</guid>
<pubDate>Sat, 07 Feb 2026 03:30:46 +0000</pubDate>
<description>Published on February 7, 2026 3:30 AM GMT I recently got a Thermal Master P1 infrared camera attachment for my phone. The goal was a house project, but it's also a great toy, especially with the kids. Getting a room pitch black but still being able to 'see' with the phone was fun for a bit. The real fun, though, was in exploring to observe all these thermal properties we'd never thought about. Here's my selfie: Light is warmer, dark is cooler. My glasses aren't cool, they're just IR-opaque.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
</channel>
</rss>