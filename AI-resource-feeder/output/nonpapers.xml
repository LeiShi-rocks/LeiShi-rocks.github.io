<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI News &amp; Posts</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI labs, blogs, and project updates</description>
<lastBuildDate>Sat, 07 Feb 2026 09:04:14 +0000</lastBuildDate>
<item>
<title>Claude Code #4: From The Before Times</title>
<link>https://www.lesswrong.com/posts/iwX2aJPKtyKAbLdip/claude-code-4-from-the-before-times</link>
<guid>title:claude code 4 from the before times</guid>
<pubDate>Fri, 06 Feb 2026 18:01:08 +0000</pubDate>
<description>Published on February 6, 2026 6:01 PM GMT Claude Opus 4. 6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5. 3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Anthropic partners with Allen Institute and Howard Hughes Medical Institute to accelerate scientific discovery</title>
<link>https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute</link>
<guid>title:anthropic partners with allen institute and howard hughes medical institute to accelerate scientific discovery</guid>
<pubDate>Sat, 07 Feb 2026 09:04:09 +0000</pubDate>
<description>Modern biological research generates data at unprecedented scale—from single-cell sequencing to whole-brain connectomics—yet transforming that data into validated biological insights remains a fundamental bottleneck. Knowledge synthesis, hypothesis generation, and experimental interpretation still depend on manual processes that can't keep pace with the data being produced. Today, Anthropic is announcing two flagship partnerships designed to close that gap. The Allen Institute and Howard Hughes Medical Institute (HHMI) will serve as founding partners in life sciences, extending Claude’s capabilities to frontier scientific research and enabling teams of scientists to work more effectively together and take on ambitious scientific challenges. Each collaboration brings together Anthropic's expertise in foundation models, agentic systems, and interpretability with world-class research institutions tackling distinct but complementary problems in biology and biomedical science. These partnerships position Claude at the center of scientific experimentation and will build a foundation in which scientists actively use Claude to plan and execute experiments.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Apple’s Xcode now supports the Claude Agent SDK</title>
<link>https://www.anthropic.com/news/apple-xcode-claude-agent-sdk</link>
<guid>title:apple s xcode now supports the claude agent sdk</guid>
<pubDate>Sat, 07 Feb 2026 09:04:08 +0000</pubDate>
<description>Apple's Xcode is where developers build, test, and distribute apps for Apple platforms, including iPhone, iPad, Mac, Apple Watch, Apple Vision Pro, and Apple TV. In September, we announced that developers would have access to Claude Sonnet 4 in Xcode 26. Claude could be used to write code, debug, and generate documentation—but it was limited to helping with individual, turn-by-turn requests. Now, Xcode 26.3 introduces a native integration with the Claude Agent SDK , the same underlying harness that powers Claude Code. Developers get the full power of Claude Code directly in Xcode—including subagents, background tasks, and plugins—all without leaving the IDE.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Introducing Claude Sonnet 4.5</title>
<link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
<guid>title:introducing claude sonnet 4 5</guid>
<pubDate>Sat, 07 Feb 2026 09:04:07 +0000</pubDate>
<description>Claude Sonnet 4. 5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Code is everywhere.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>In (highly contingent!) defense of interpretability-in-the-loop ML training</title>
<link>https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop</link>
<guid>title:in highly contingent defense of interpretability in the loop ml training</guid>
<pubDate>Fri, 06 Feb 2026 16:32:27 +0000</pubDate>
<description>Published on February 6, 2026 4:32 PM GMT Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function. Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&amp;nbsp; Yudkowsky 2022 : When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect. &amp;nbsp; Optimizing against an interpreted thought optimizes against interpretability. Or&amp;nbsp; Zvi 2025 : The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M].</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>world-models</category>
</item>
<item>
<title>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title>
<link>https://www.lesswrong.com/posts/dTfpSfTfYs7qg4MFi/data-centric-interpretability-for-llm-based-multi-agent</link>
<guid>title:data centric interpretability for llm based multi agent reinforcement learning</guid>
<pubDate>Fri, 06 Feb 2026 19:27:09 +0000</pubDate>
<description>Published on February 6, 2026 7:27 PM GMT TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputs paper Abstract Large language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14. 2%.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Spectral Signatures of Gradual Disempowerment</title>
<link>https://www.lesswrong.com/posts/erTAgDriWAw3evecP/spectral-signatures-of-gradual-disempowerment</link>
<guid>title:spectral signatures of gradual disempowerment</guid>
<pubDate>Fri, 06 Feb 2026 15:08:08 +0000</pubDate>
<description>Published on February 6, 2026 3:08 PM GMT TL;DR AI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance. Introduction AI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases. The stubborn part is that it operates across institutional boundaries simultaneously.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>diffusion</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>serving</category>
<category>vision</category>
</item>
<item>
<title>ServiceNow chooses Claude to power customer apps and increase internal productivity</title>
<link>https://www.anthropic.com/news/servicenow-anthropic-claude</link>
<guid>title:servicenow chooses claude to power customer apps and increase internal productivity</guid>
<pubDate>Sat, 07 Feb 2026 09:04:09 +0000</pubDate>
<description>As enterprises move beyond experimenting with AI and start putting it into production across their core business operations, scale and security matters just as much as capabilities. With this in mind, ServiceNow, which helps large companies manage and automate everything from IT support to HR to customer service on a single platform, has chosen Claude as its default model for its ServiceNow Build Agent, and as a preferred model across the ServiceNow AI Platform. Build Agent helps developers of all skill levels build and use Claude as an “agent” that can reason, decide on actions, and execute tasks autonomously. In addition, ServiceNow is rolling out Claude and Claude Code across its global workforce of more than 29,000 employees to streamline sales preparation—cutting seller preparation time by 95% and boosting engineering productivity with Claude Code to reduce the time between idea and implementation across the organization.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Sat, 07 Feb 2026 09:04:06 +0000</pubDate>
<description>We’re upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Sat, 07 Feb 2026 09:04:05 +0000</pubDate>
<description>We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. The first AI-assisted drive on another planet.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Haiku 4.5</title>
<link>https://www.anthropic.com/news/claude-haiku-4-5</link>
<guid>title:introducing claude haiku 4 5</guid>
<pubDate>Sat, 07 Feb 2026 09:04:08 +0000</pubDate>
<description>Claude Haiku 4. 5, our latest small model, is available today to all users. What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4. 5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Sat, 07 Feb 2026 09:04:07 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Robust Finite Policies are Nontrivially Structured</title>
<link>https://www.lesswrong.com/posts/ieX8nK2b2i4JDRH5s/robust-finite-policies-are-nontrivially-structured</link>
<guid>title:robust finite policies are nontrivially structured</guid>
<pubDate>Fri, 06 Feb 2026 17:52:22 +0000</pubDate>
<description>Published on February 6, 2026 5:47 PM GMT This post was created during the Dovetail Research Fellowship. Thanks to Alex , Alfred , &amp;nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions. Overview The proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem , which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone. For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature. We begin by defining every part of the framework.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>agents</category>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
<category>world-models</category>
</item>
<item>
<title>Playing with an Infrared Camera</title>
<link>https://www.lesswrong.com/posts/ALSFkR23qjWjctyhg/playing-with-an-infrared-camera</link>
<guid>title:playing with an infrared camera</guid>
<pubDate>Sat, 07 Feb 2026 03:30:46 +0000</pubDate>
<description>Published on February 7, 2026 3:30 AM GMT I recently got a Thermal Master P1 infrared camera attachment for my phone. The goal was a house project, but it's also a great toy, especially with the kids. Getting a room pitch black but still being able to 'see' with the phone was fun for a bit. The real fun, though, was in exploring to observe all these thermal properties we'd never thought about. Here's my selfie: Light is warmer, dark is cooler. My glasses aren't cool, they're just IR-opaque.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Honey, I shrunk the brain</title>
<link>https://www.lesswrong.com/posts/KvbBYaKmGcJKvvWd8/honey-i-shrunk-the-brain</link>
<guid>doi:10.3389/fmedt.2024.1400615/full</guid>
<pubDate>Sat, 07 Feb 2026 00:01:47 +0000</pubDate>
<description>Published on February 7, 2026 12:01 AM GMT When cryoprotectants are perfused through the blood vessels in the brain, they cannot cross the blood-brain barrier as fast as water can move in the opposite direction. And cryoprotectants generally have a much higher osmotic concentration than the typical blood plasma. For example, the cryoprotectant solution M22 has an osmotic concentration around 100 times higher. As a result, in a successful cryoprotectant perfusion (without fixatives), water rushes out of the tissue into the blood vessels, the tissue dehydrates, and you end up with a shrunken brain that is visibly pulled away from the skull. The brain weight goes down by 50% or more. This is currently considered a good sign of cryoprotectant perfusion quality.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Strategy of von Neumann and strategy of Rosenbergs</title>
<link>https://www.lesswrong.com/posts/orJPh4QdCicDt7c3E/strategy-of-von-neumann-and-strategy-of-rosenbergs</link>
<guid>title:strategy of von neumann and strategy of rosenbergs</guid>
<pubDate>Fri, 06 Feb 2026 22:50:12 +0000</pubDate>
<description>Published on February 6, 2026 10:50 PM GMT This is not a call for espionage, but an analysis of another strategy Von Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project). My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them.</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Parks Aren't Nature</title>
<link>https://www.lesswrong.com/posts/cjxPFxAe5WRKA6SeF/parks-aren-t-nature</link>
<guid>title:parks aren t nature</guid>
<pubDate>Fri, 06 Feb 2026 18:27:05 +0000</pubDate>
<description>Published on February 6, 2026 6:27 PM GMT I. I love dogs. I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy! ”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this. Why do I love dogs?</description>
<source url="https://www.lesswrong.com/feed.xml">lesswrong</source>
<category>alignment</category>
<category>lesswrong</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Natively Adaptive Interfaces: A new framework for AI accessibility</title>
<link>https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/</link>
<guid>title:natively adaptive interfaces a new framework for ai accessibility</guid>
<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>accessibility</category>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference—far beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>How Google Cloud is helping Team USA elevate their tricks with AI</title>
<link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/</link>
<guid>title:how google cloud is helping team usa elevate their tricks with ai</guid>
<pubDate>Thu, 05 Feb 2026 16:00:00 +0000</pubDate>
<description>A woman outdoors in the snow looks at a tablet. A half pipe is behind her.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google cloud</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
</channel>
</rss>