<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI News &amp; Posts</title>
<link>https://example.com/ai-trend-feed</link>
<description>Top daily AI labs, blogs, and project updates</description>
<lastBuildDate>Wed, 11 Feb 2026 09:02:06 +0000</lastBuildDate>
<item>
<title>Anthropic partners with Allen Institute and Howard Hughes Medical Institute to accelerate scientific discovery</title>
<link>https://www.anthropic.com/news/anthropic-partners-with-allen-institute-and-howard-hughes-medical-institute</link>
<guid>title:anthropic partners with allen institute and howard hughes medical institute to accelerate scientific discovery</guid>
<pubDate>Wed, 11 Feb 2026 09:01:59 +0000</pubDate>
<description>Modern biological research generates data at unprecedented scale‚Äîfrom single-cell sequencing to whole-brain connectomics‚Äîyet transforming that data into validated biological insights remains a fundamental bottleneck. Knowledge synthesis, hypothesis generation, and experimental interpretation still depend on manual processes that can't keep pace with the data being produced. Today, Anthropic is announcing two flagship partnerships designed to close that gap. The Allen Institute and Howard Hughes Medical Institute (HHMI) will serve as founding partners in life sciences, extending Claude‚Äôs capabilities to frontier scientific research and enabling teams of scientists to work more effectively together and take on ambitious scientific challenges. Each collaboration brings together Anthropic's expertise in foundation models, agentic systems, and interpretability with world-class research institutions tackling distinct but complementary problems in biology and biomedical science. These partnerships position Claude at the center of scientific experimentation and will build a foundation in which scientists actively use Claude to plan and execute experiments.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>Apple‚Äôs Xcode now supports the Claude Agent SDK</title>
<link>https://www.anthropic.com/news/apple-xcode-claude-agent-sdk</link>
<guid>title:apple s xcode now supports the claude agent sdk</guid>
<pubDate>Wed, 11 Feb 2026 09:01:59 +0000</pubDate>
<description>Apple's Xcode is where developers build, test, and distribute apps for Apple platforms, including iPhone, iPad, Mac, Apple Watch, Apple Vision Pro, and Apple TV. In September, we announced that developers would have access to Claude Sonnet 4 in Xcode 26. Claude could be used to write code, debug, and generate documentation‚Äîbut it was limited to helping with individual, turn-by-turn requests. Now, Xcode 26.3 introduces a native integration with the Claude Agent SDK , the same underlying harness that powers Claude Code. Developers get the full power of Claude Code directly in Xcode‚Äîincluding subagents, background tasks, and plugins‚Äîall without leaving the IDE.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>rl</category>
<category>safety</category>
<category>vision</category>
</item>
<item>
<title>Introducing Claude Sonnet 4.5</title>
<link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
<guid>title:introducing claude sonnet 4 5</guid>
<pubDate>Wed, 11 Feb 2026 09:01:58 +0000</pubDate>
<description>Claude Sonnet 4. 5 is the best coding model in the world. It's the strongest model for building complex agents. It‚Äôs the best model at using computers. And it shows substantial gains in reasoning and math. Code is everywhere.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>reasoning</category>
<category>rl</category>
<category>safety</category>
</item>
<item>
<title>ServiceNow chooses Claude to power customer apps and increase internal productivity</title>
<link>https://www.anthropic.com/news/servicenow-anthropic-claude</link>
<guid>title:servicenow chooses claude to power customer apps and increase internal productivity</guid>
<pubDate>Wed, 11 Feb 2026 09:02:02 +0000</pubDate>
<description>As enterprises move beyond experimenting with AI and start putting it into production across their core business operations, scale and security matters just as much as capabilities. With this in mind, ServiceNow, which helps large companies manage and automate everything from IT support to HR to customer service on a single platform, has chosen Claude as its default model for its ServiceNow Build Agent, and as a preferred model across the ServiceNow AI Platform. Build Agent helps developers of all skill levels build and use Claude as an ‚Äúagent‚Äù that can reason, decide on actions, and execute tasks autonomously. In addition, ServiceNow is rolling out Claude and Claude Code across its global workforce of more than 29,000 employees to streamline sales preparation‚Äîcutting seller preparation time by 95% and boosting engineering productivity with Claude Code to reduce the time between idea and implementation across the organization.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Opus 4.6</title>
<link>https://www.anthropic.com/news/claude-opus-4-6</link>
<guid>title:introducing claude opus 4 6</guid>
<pubDate>Wed, 11 Feb 2026 09:01:58 +0000</pubDate>
<description>We‚Äôre upgrading our smartest model. The new Claude Opus 4. 6 improves on its predecessor‚Äôs coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4. 6 features a 1M token context window in beta 1 .</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>R¬≤D¬≤: Scaling Multimodal Robot Learning with NVIDIA Isaac Lab</title>
<link>https://developer.nvidia.com/blog/r2d2-scaling-multimodal-robot-learning-with-nvidia-isaac-lab/</link>
<guid>title:r d scaling multimodal robot learning with nvidia isaac lab</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Building robust, intelligent robots requires testing them in complex environments. However, gathering data in the physical world is expensive, slow, and often...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>ai foundation models</category>
<category>hardware</category>
<category>humanoid robots</category>
<category>infra</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>nvidia research</category>
<category>open source</category>
<category>physical ai</category>
<category>rl</category>
<category>robotics</category>
<category>robotics research and development digest (r¬≤d¬≤)</category>
<category>simulation / modeling / design</category>
<category>training</category>
</item>
<item>
<title>Newsroom</title>
<link>https://www.anthropic.com/news</link>
<guid>title:newsroom</guid>
<pubDate>Wed, 11 Feb 2026 09:01:57 +0000</pubDate>
<description>We‚Äôre upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4. 6 is an industry-leading model, often by wide margin. We‚Äôve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. The first AI-assisted drive on another planet.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>agents</category>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Introducing Claude Haiku 4.5</title>
<link>https://www.anthropic.com/news/claude-haiku-4-5</link>
<guid>title:introducing claude haiku 4 5</guid>
<pubDate>Wed, 11 Feb 2026 09:01:59 +0000</pubDate>
<description>Claude Haiku 4. 5, our latest small model, is available today to all users. What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4. 5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Claude is a space to think</title>
<link>https://www.anthropic.com/news/claude-is-a-space-to-think</link>
<guid>title:claude is a space to think</guid>
<pubDate>Wed, 11 Feb 2026 09:01:58 +0000</pubDate>
<description>There are many good places for advertising. A conversation with Claude is not one of them. Advertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We‚Äôve run our own ad campaigns , and our AI models have, in turn, helped many of our customers in the advertising industry. But including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.</description>
<source url="https://www.anthropic.com/news">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>safety</category>
</item>
<item>
<title>Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations</title>
<link>https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/</link>
<guid>title:beyond one on one authoring simulating and testing dynamic human ai group conversations</guid>
<pubDate>Tue, 10 Feb 2026 18:30:00 +0000</pubDate>
<description>Human-Computer Interaction and Visualization</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>google-research</category>
<category>human-computer interaction and visualization</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>9 fun questions to try asking Google Photos</title>
<link>https://blog.google/products-and-platforms/products/photos/ask-button-ask-photos-tips/</link>
<guid>title:9 fun questions to try asking google photos</guid>
<pubDate>Tue, 10 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of outdoor images, a blue icon that say &quot;Ask Photos,&quot; and examples of Ask Photos prompts.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Helping kids and teens learn and grow online on Safer Internet Day</title>
<link>https://blog.google/innovation-and-ai/technology/safety-security/safer-internet-day-2026-kids-teens/</link>
<guid>title:helping kids and teens learn and grow online on safer internet day</guid>
<pubDate>Tue, 10 Feb 2026 02:30:00 +0000</pubDate>
<description>User profile on smartphone connected to security, media, and settings icons.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>safety &amp; security</category>
</item>
<item>
<title>Automating Inference Optimizations with NVIDIA TensorRT LLM AutoDeploy</title>
<link>https://developer.nvidia.com/blog/automating-inference-optimizations-with-nvidia-tensorrt-llm-autodeploy/</link>
<guid>title:automating inference optimizations with nvidia tensorrt llm autodeploy</guid>
<pubDate>Mon, 09 Feb 2026 18:30:00 +0000</pubDate>
<description>NVIDIA TensorRT LLM enables developers to build high-performance inference engines for large language models (LLMs), but deploying a new architecture...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai inference</category>
<category>developer tools &amp; techniques</category>
<category>hardware</category>
<category>inference performance</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>mlops</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>pytorch</category>
<category>serving</category>
<category>training</category>
</item>
<item>
<title>How AI trained on birds is surfacing underwater mysteries</title>
<link>https://research.google/blog/how-ai-trained-on-birds-is-surfacing-underwater-mysteries/</link>
<guid>title:how ai trained on birds is surfacing underwater mysteries</guid>
<pubDate>Mon, 09 Feb 2026 18:38:06 +0000</pubDate>
<description>Climate &amp; Sustainability</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>climate &amp; sustainability</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>open source models &amp; datasets</category>
<category>research</category>
<category>science</category>
<category>sound &amp; accoustics</category>
</item>
<item>
<title>Natively Adaptive Interfaces: A new framework for AI accessibility</title>
<link>https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/</link>
<guid>title:natively adaptive interfaces a new framework for ai accessibility</guid>
<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
<description>A collage of four images, the first of a woman with curly hair in front of a silver laptop, the second of the same woman and a man with short black hair speaking on a stairwell, the third of a the same man with glasses, and an aerial image of NTID</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>accessibility</category>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>google.org</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Build with Kimi K2.5 Multimodal VLM Using NVIDIA GPU-Accelerated Endpoints</title>
<link>https://developer.nvidia.com/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/</link>
<guid>title:build with kimi k2 5 multimodal vlm using nvidia gpu accelerated endpoints</guid>
<pubDate>Wed, 04 Feb 2026 19:46:33 +0000</pubDate>
<description>Kimi K2.5 is the newest open vision language model (VLM) from the Kimi family of models. Kimi K2.5 is a general-purpose multimodal model that excels in current...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai agent</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>top stories</category>
<category>training</category>
<category>vision</category>
<category>vlms</category>
</item>
<item>
<title>3 Ways NVFP4 Accelerates AI Training and Inference</title>
<link>https://developer.nvidia.com/blog/3-ways-nvfp4-accelerates-ai-training-and-inference/</link>
<guid>title:3 ways nvfp4 accelerates ai training and inference</guid>
<pubDate>Fri, 06 Feb 2026 16:00:00 +0000</pubDate>
<description>The latest AI models continue to grow in size and complexity, demanding increasing amounts of compute performance for training and inference‚Äîfar beyond what...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>blackwell</category>
<category>blackwell ultra</category>
<category>data center / cloud</category>
<category>featured</category>
<category>gb300</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvfp4</category>
<category>nvidia</category>
<category>rubin</category>
<category>serving</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>How Google Cloud is helping Team USA elevate their tricks with AI</title>
<link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/</link>
<guid>title:how google cloud is helping team usa elevate their tricks with ai</guid>
<pubDate>Thu, 05 Feb 2026 16:00:00 +0000</pubDate>
<description>A woman outdoors in the snow looks at a tablet. A half pipe is behind her.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google cloud</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Watch our new Gemini ad ahead of football‚Äôs biggest weekend</title>
<link>https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/</link>
<guid>title:watch our new gemini ad ahead of football s biggest weekend</guid>
<pubDate>Thu, 05 Feb 2026 14:30:00 +0000</pubDate>
<description>A toddler in a blue and yellow striped shirt sits on a kitchen counter eating a red apple. Text in the corner reads: 'New Home, Google Gemini SB Commercial‚Äô</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>gemini</category>
<category>google</category>
<category>lab</category>
<category>nonpaper</category>
<category>photos</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>How AI tools can redefine universal design to increase accessibility</title>
<link>https://research.google/blog/how-ai-agents-can-redefine-universal-design-to-increase-accessibility/</link>
<guid>title:how ai tools can redefine universal design to increase accessibility</guid>
<pubDate>Thu, 05 Feb 2026 08:28:00 +0000</pubDate>
<description>Education Innovation</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>education innovation</category>
<category>google-research</category>
<category>machine intelligence</category>
<category>models</category>
<category>natural language processing</category>
<category>nonpaper</category>
<category>research</category>
<category>responsible ai</category>
<category>science</category>
</item>
<item>
<title>Collaborating on a nationwide randomized study of AI in real-world virtual care</title>
<link>https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care/</link>
<guid>title:collaborating on a nationwide randomized study of ai in real world virtual care</guid>
<pubDate>Tue, 03 Feb 2026 18:15:01 +0000</pubDate>
<description>Generative AI</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>generative ai</category>
<category>google-research</category>
<category>health &amp; bioscience</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>rl</category>
<category>science</category>
</item>
<item>
<title>The latest AI news we announced in January</title>
<link>https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/</link>
<guid>title:the latest ai news we announced in january</guid>
<pubDate>Wed, 04 Feb 2026 16:55:00 +0000</pubDate>
<description>mp4 showing a carousel of images including a card reading &quot;Help that's made for you&quot;</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>arts &amp; culture</category>
<category>chrome</category>
<category>developer tools</category>
<category>gemini</category>
<category>gemini app</category>
<category>gmail</category>
<category>google</category>
<category>google ads</category>
<category>google cloud</category>
<category>google deepmind</category>
<category>lab</category>
<category>learning &amp; education</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>search</category>
<category>shopping</category>
</item>
<item>
<title>‚ÄãSequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
<link>https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/</link>
<guid>title:sequential attention making ai models leaner and faster without sacrificing accuracy</guid>
<pubDate>Wed, 04 Feb 2026 15:14:00 +0000</pubDate>
<description>Algorithms &amp; Theory</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>algorithms &amp; theory</category>
<category>google-research</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>How to Build a Document Processing Pipeline for RAG with Nemotron</title>
<link>https://developer.nvidia.com/blog/how-to-build-a-document-processing-pipeline-for-rag-with-nemotron/</link>
<guid>title:how to build a document processing pipeline for rag with nemotron</guid>
<pubDate>Wed, 04 Feb 2026 16:00:00 +0000</pubDate>
<description>What if your AI agent could instantly parse complex PDFs, extract nested tables, and &quot;see&quot; data within charts as easily as reading a text file? With NVIDIA...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>agents</category>
<category>build ai agents</category>
<category>data science</category>
<category>developer tools &amp; techniques</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm techniques</category>
<category>llms</category>
<category>nemo</category>
<category>nemo retriever</category>
<category>nemotron</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>retrieval augmented generation (rag)</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>Hear more about interactive world models in our latest podcast.</title>
<link>https://blog.google/innovation-and-ai/technology/ai/release-notes-podcast-project-genie/</link>
<guid>title:hear more about interactive world models in our latest podcast</guid>
<pubDate>Thu, 29 Jan 2026 15:00:00 +0000</pubDate>
<description>Project Genie: Create and explore worlds</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google deepmind</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
<category>world-models</category>
</item>
<item>
<title>How to Build License-Compliant Synthetic Data Pipelines for AI Model Distillation</title>
<link>https://developer.nvidia.com/blog/how-to-build-license-compliant-synthetic-data-pipelines-for-ai-model-distillation/</link>
<guid>title:how to build license compliant synthetic data pipelines for ai model distillation</guid>
<pubDate>Thu, 05 Feb 2026 18:00:00 +0000</pubDate>
<description>Specialized AI models are built to perform specific tasks or solve particular problems. But if you‚Äôve ever tried to fine-tune or distill a domain-specific...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llms</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>open source</category>
<category>pandas</category>
<category>synthetic data generation</category>
<category>training</category>
<category>training ai models</category>
</item>
<item>
<title>How Painkiller RTX Uses Generative AI to Modernize Game Assets at Scale</title>
<link>https://developer.nvidia.com/blog/how-painkiller-rtx-uses-generative-ai-to-modernize-game-assets-at-scale/</link>
<guid>title:how painkiller rtx uses generative ai to modernize game assets at scale</guid>
<pubDate>Thu, 05 Feb 2026 14:00:00 +0000</pubDate>
<description>Painkiller RTX sets a new standard for how small teams can balance massive visual ambition with limited resources by integrating generative AI. By upscaling...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>content creation / rendering</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>news</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>How we‚Äôre helping preserve the genetic information of endangered species with AI</title>
<link>https://blog.google/innovation-and-ai/technology/ai/ai-to-preserve-endangered-species/</link>
<guid>title:how we re helping preserve the genetic information of endangered species with ai</guid>
<pubDate>Mon, 02 Feb 2026 18:00:00 +0000</pubDate>
<description>A four-part vertical collage showing a cotton-top tamarin, an ibex, a golden lion tamarin, and a penguin.</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google research</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Advancing AI benchmarking with Game Arena</title>
<link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/</link>
<guid>title:advancing ai benchmarking with game arena</guid>
<pubDate>Mon, 02 Feb 2026 17:00:00 +0000</pubDate>
<description>An illustration of a King and Ace playing card, a wolf's head, two chess pieces, a poker chip, and other abstract shapes on a white background.1</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google deepmind</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
</item>
<item>
<title>Accelerating Long-Context Model Training in JAX and XLA</title>
<link>https://developer.nvidia.com/blog/accelerating-long-context-model-training-in-jax-and-xla/</link>
<guid>title:accelerating long context model training in jax and xla</guid>
<pubDate>Tue, 03 Feb 2026 17:30:00 +0000</pubDate>
<description>Large language models (LLMs) are rapidly expanding their context windows, with recent models supporting sequences of 128K tokens, 256K tokens, and beyond....</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>cuda graphs</category>
<category>developer tools &amp; techniques</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>llm techniques</category>
<category>networking / communications</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
<category>training ai models</category>
</item>
<item>
<title>Project Genie: Experimenting with infinite, interactive worlds</title>
<link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/</link>
<guid>title:project genie experimenting with infinite interactive worlds</guid>
<pubDate>Thu, 29 Jan 2026 17:00:00 +0000</pubDate>
<description>Text reads Introducing Project Genie</description>
<source url="https://blog.google/technology/ai/rss/">google</source>
<category>ai</category>
<category>google</category>
<category>google deepmind</category>
<category>google one</category>
<category>lab</category>
<category>nonpaper</category>
<category>product</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Updating Classifier Evasion for Vision Language Models</title>
<link>https://developer.nvidia.com/blog/updating-classifier-evasion-for-vision-language-models/</link>
<guid>title:updating classifier evasion for vision language models</guid>
<pubDate>Wed, 28 Jan 2026 16:19:12 +0000</pubDate>
<description>Advances in AI architectures have unlocked multimodal functionality, enabling transformer models to process multiple forms of data in the same context. For...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>ai red team</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>multimodal</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>security for ai</category>
<category>training</category>
<category>trustworthy ai / cybersecurity</category>
<category>vision</category>
<category>vlms</category>
</item>
<item>
<title>Optimizing Communication for Mixture-of-Experts Training with Hybrid Expert Parallel</title>
<link>https://developer.nvidia.com/blog/optimizing-communication-for-mixture-of-experts-training-with-hybrid-expert-parallel/</link>
<guid>title:optimizing communication for mixture of experts training with hybrid expert parallel</guid>
<pubDate>Mon, 02 Feb 2026 18:43:08 +0000</pubDate>
<description>In LLM training, Expert Parallel (EP) communication for hyperscale mixture-of-experts (MoE) models is challenging. EP communication is essentially all-to-all,...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>data center / cloud</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>networking / communications</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Towards a science of scaling agent systems: When and why agent systems work</title>
<link>https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/</link>
<guid>title:towards a science of scaling agent systems when and why agent systems work</guid>
<pubDate>Wed, 28 Jan 2026 11:00:00 +0000</pubDate>
<description>Generative AI</description>
<source url="https://research.google/blog/rss/">google-research</source>
<category>agents</category>
<category>generative ai</category>
<category>google-research</category>
<category>machine intelligence</category>
<category>models</category>
<category>nonpaper</category>
<category>research</category>
<category>science</category>
</item>
<item>
<title>Advancing GPU Programming with the CUDA Tile IR Backend for OpenAI Triton</title>
<link>https://developer.nvidia.com/blog/advancing-gpu-programming-with-the-cuda-tile-ir-backend-for-openai-triton/</link>
<guid>title:advancing gpu programming with the cuda tile ir backend for openai triton</guid>
<pubDate>Fri, 30 Jan 2026 20:01:47 +0000</pubDate>
<description>NVIDIA CUDA Tile is a GPU-based programming model that targets portability for NVIDIA Tensor Cores, unlocking peak GPU performance. One of the great things...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>cuda tile</category>
<category>data science</category>
<category>developer tools &amp; techniques</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>rl</category>
<category>top stories</category>
<category>training</category>
</item>
<item>
<title>Practical Security Guidance for Sandboxing Agentic Workflows and Managing Execution Risk</title>
<link>https://developer.nvidia.com/blog/practical-security-guidance-for-sandboxing-agentic-workflows-and-managing-execution-risk/</link>
<guid>title:practical security guidance for sandboxing agentic workflows and managing execution risk</guid>
<pubDate>Fri, 30 Jan 2026 16:13:00 +0000</pubDate>
<description>AI coding agents enable developers to work faster by streamlining tasks and driving automated, test-driven development. However, they also introduce a...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>agents</category>
<category>ai agent</category>
<category>ai red team</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>top stories</category>
<category>training</category>
<category>trustworthy ai / cybersecurity</category>
</item>
<item>
<title>Speeding Up Variable-Length Training with Dynamic Context Parallelism and NVIDIA Megatron Core</title>
<link>https://developer.nvidia.com/blog/speeding-up-variable-length-training-with-dynamic-context-parallelism-and-nvidia-megatron-core/</link>
<guid>title:speeding up variable length training with dynamic context parallelism and nvidia megatron core</guid>
<pubDate>Wed, 28 Jan 2026 16:28:06 +0000</pubDate>
<description>This post introduces Dynamic Context Parallelism (Dynamic-CP), a scheduling approach in NVIDIA Megatron Core used for LLM post-training or DiT pre-training. It...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>llm</category>
<category>llms</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Establishing a Scalable Sparse Ecosystem with the Universal Sparse Tensor</title>
<link>https://developer.nvidia.com/blog/establishing-a-scalable-sparse-ecosystem-with-the-universal-sparse-tensor/</link>
<guid>title:establishing a scalable sparse ecosystem with the universal sparse tensor</guid>
<pubDate>Fri, 30 Jan 2026 18:00:00 +0000</pubDate>
<description>Sparse tensors are vectors, matrices, and higher-dimensional generalizations with many zeros. They are crucial in various fields such as scientific computing,...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>deep learning</category>
<category>developer tools &amp; techniques</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>python</category>
<category>training</category>
</item>
<item>
<title>Ensuring Balanced GPU Allocation in Kubernetes Clusters with Time-Based Fairshare</title>
<link>https://developer.nvidia.com/blog/ensuring-balanced-gpu-allocation-in-kubernetes-clusters-with-time-based-fairshare/</link>
<guid>title:ensuring balanced gpu allocation in kubernetes clusters with time based fairshare</guid>
<pubDate>Wed, 28 Jan 2026 17:00:00 +0000</pubDate>
<description>NVIDIA Run:ai v2.24 introduces time-based fairshare, a new scheduling mode that brings fair-share scheduling with time awareness for over-quota resources to...</description>
<source url="https://developer.nvidia.com/blog/feed">nvidia</source>
<category>agentic ai / generative ai</category>
<category>data center / cloud</category>
<category>featured</category>
<category>hardware</category>
<category>infra</category>
<category>kubernetes</category>
<category>nonpaper</category>
<category>nvidia</category>
<category>training</category>
</item>
<item>
<title>Building a Reliable AI Analytics Agent with BigQuery, a Semantic Layer, and Google ADK</title>
<link>https://pub.towardsai.net/building-a-reliable-ai-analytics-agent-with-bigquery-a-semantic-layer-and-google-adk-9cd6cfbef218?source=rss----98111c9905da---4</link>
<guid>title:building a reliable ai analytics agent with bigquery a semantic layer and google adk</guid>
<pubDate>Wed, 11 Feb 2026 03:19:32 +0000</pubDate>
<description>How to prevent your LLM from confidently giving you the wrong numbers You‚Äôve probably seen it: an LLM generates impressively complex SQL queries in seconds, executes them flawlessly, and returns results that look completely reasonable. Your stakeholders are amazed. Your team is excited. And then someone checks the numbers manually. They‚Äôre wrong. Not obviously wrong , subtly, dangerously wrong.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agentic-ai</category>
<category>agents</category>
<category>ai</category>
<category>bigquery</category>
<category>google-adk</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>reasoning</category>
<category>rl</category>
<category>serving</category>
</item>
<item>
<title>The Role of Signal To Noise in Loss Convergence</title>
<link>https://pub.towardsai.net/the-role-of-signal-to-noise-in-loss-convergence-50414687efd1?source=rss----98111c9905da---4</link>
<guid>title:the role of signal to noise in loss convergence</guid>
<pubDate>Tue, 10 Feb 2026 22:01:01 +0000</pubDate>
<description>The Role of Signal-to-Noise in Loss Convergence Source: Image by author Consider the normal NLP training curve during pre-training. That nice beautiful line of healthy training. Why does it behave like that? Why does the image above not behave like that? There are two factors that determine the kind of line we will get out of training. The signal to noise ratio, and the size of the generalized solution space.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>ai</category>
<category>llm</category>
<category>machine-learning</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>reasoning</category>
<category>reinforcement-learning</category>
<category>rl</category>
<category>vision</category>
<category>world-models</category>
</item>
<item>
<title>GLM-5 Leaked? New Pony Alpha Stealth Model IS INSANE! Opus 4.6 Quality!</title>
<link>https://www.youtube.com/watch?v=MfJXFF1-o0E</link>
<guid>title:glm 5 leaked new pony alpha stealth model is insane opus 4 6 quality</guid>
<pubDate>Wed, 11 Feb 2026 04:13:20 +0000</pubDate>
<description>GLM-5 might have just leaked‚Ä¶ and the new Pony Alpha stealth model is absolutely insane. ü§Ø In this video, we take a deep dive into Pony Alpha, a mysterious new model showing up on OpenRouter that many believe is actually GLM-5 in disguise. With a massive 200K context window, strong performance in coding, reasoning, agentic workflows, and UI generation, this model is starting to look dangerously close to Opus 4. 6 quality. üîó My Links: Sponsor a Video or Do a Demo of Your Product, Contact me: intheworldzofai@gmail. com üî• Become a Patron (Private Discord): https://patreon.</description>
<source url="https://www.youtube.com/feeds/videos.xml?channel_id=UC2WmuBuFq6gL08QYG-JjXKw">youtube.com</source>
<category>agents</category>
<category>ai</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>rl</category>
<category>youtube.com</category>
</item>
<item>
<title>Introducing Showboat and Rodney, so agents can demo what they‚Äôve built</title>
<link>https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#atom-everything</link>
<guid>title:introducing showboat and rodney so agents can demo what they ve built</guid>
<pubDate>Tue, 10 Feb 2026 17:45:29 +0000</pubDate>
<description>A key challenge working with coding agents is having them both test what they‚Äôve built and demonstrate that software to you, their overseer. This goes beyond automated tests - we need artifacts that show their progress and help us see exactly what the agent-produced software is able to do. I‚Äôve just released two new tools aimed at this problem: Showboat and Rodney . Proving code actually works Showboat: Agents build documents to demo their work Rodney: CLI browser automation designed to work with Showboat Test-driven development helps, but we still need manual testing I built both of these tools on my phone Proving code actually works I recently wrote about how the job of a software engineer isn't to write code, it's to deliver code that works . A big part of that is proving to ourselves and to other people that the code we are responsible for behaves as expected. This becomes even more important - and challenging - as we embrace coding agents as a core part of our software development process.</description>
<source url="https://simonwillison.net/atom/everything/">simonwillison</source>
<category>agents</category>
<category>ai</category>
<category>ai-assisted-programming</category>
<category>async-coding-agents</category>
<category>coding-agents</category>
<category>engineering</category>
<category>generative-ai</category>
<category>go</category>
<category>llm</category>
<category>llms</category>
<category>markdown</category>
<category>nonpaper</category>
<category>projects</category>
<category>rl</category>
<category>robotics</category>
<category>simonwillison</category>
<category>testing</category>
<category>tools</category>
</item>
<item>
<title>Project Vend: Phase two</title>
<link>https://www.anthropic.com/research/project-vend-2</link>
<guid>title:project vend phase two</guid>
<pubDate>Wed, 11 Feb 2026 09:01:55 +0000</pubDate>
<description>In June, we revealed that we‚Äôd set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend , a free-form experiment exploring how well AIs could do on complex, real-world tasks. Alas, the shopkeeper‚Äîa modified version of Claude we named ‚ÄúClaudius‚Äù‚Äîdid not do particularly well. It lost money over time, had a strange identity crisis where it claimed it was a human wearing a blue blazer, and was goaded by mischievous Anthropic employees into selling products (particularly, for some reason, tungsten cubes) at a substantial loss. But the capabilities of large language models in areas like reasoning, writing, coding, and much else besides are increasing at a breathless pace. Has Claudius‚Äôs ‚Äúrunning a shop‚Äù capability shown the same improvement?</description>
<source url="https://www.anthropic.com/research">anthropic</source>
<category>anthropic</category>
<category>lab</category>
<category>llm</category>
<category>models</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>research</category>
<category>rl</category>
</item>
<item>
<title>Calendar Feeds: Where It All Started</title>
<link>https://dev.to/alistairjcbrown/calendar-feeds-where-it-all-started-27o2</link>
<guid>title:calendar feeds where it all started</guid>
<pubDate>Wed, 11 Feb 2026 08:34:00 +0000</pubDate>
<description>When I lived in Belfast, I had one problem: I wanted to know what was showing at the Strand Cinema without having to remember to check their website. I wanted to look at next Friday in my calendar and see if there was anything worth going to. So I built a scraper. Pull the listings, transform them into something structured, generate an ICS file. Done. That was June 2023.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>ai</category>
<category>dev.to</category>
<category>javascript</category>
<category>llm</category>
<category>news</category>
<category>nonpaper</category>
<category>opensource</category>
<category>rl</category>
<category>serving</category>
<category>webdev</category>
<category>webscraping</category>
</item>
<item>
<title>Sandboxing AI Coding Agents with Devcontainers</title>
<link>https://dev.to/klaus82/sandboxing-ai-coding-agents-with-devcontainers-4ja3</link>
<guid>title:sandboxing ai coding agents with devcontainers</guid>
<pubDate>Wed, 11 Feb 2026 08:28:13 +0000</pubDate>
<description>The rise of AI-powered coding assistants like GitHub Copilot, Cursor, and various autonomous agents built on Claude or GPT-4 has fundamentally changed how we write code. These tools can read your codebase, suggest changes, and even execute commands. This capability is powerful, but it also raises a big security concern: you're giving an AI system access to your development environment. The question isn't whether these tools are malicious; it's about implementing proper isolation as a matter of principle. Unlike standalone command-line tools that you invoke explicitly, IDE-integrated AI agents run continuously in the background. GitHub Copilot, for instance, analyzes your code as you type, maintains context across files, and can access any file your editor can see.</description>
<source url="https://dev.to/feed">dev.to</source>
<category>agents</category>
<category>ai</category>
<category>containers</category>
<category>dev.to</category>
<category>news</category>
<category>nonpaper</category>
<category>rl</category>
<category>security</category>
<category>serving</category>
</item>
<item>
<title>NVIDIA Researchers Introduce KVTC Transform Coding Pipeline to Compress Key-Value Caches by 20x for Efficient LLM Serving</title>
<link>https://www.marktechpost.com/2026/02/10/nvidia-researchers-introduce-kvtc-transform-coding-pipeline-to-compress-key-value-caches-by-20x-for-efficient-llm-serving/</link>
<guid>title:nvidia researchers introduce kvtc transform coding pipeline to compress key value caches by 20x for efficient llm serving</guid>
<pubDate>Wed, 11 Feb 2026 04:38:57 +0000</pubDate>
<description>Serving Large Language Models (LLMs) at scale is a massive engineering challenge because of Key-Value (KV) cache management. As models grow in size and reasoning capability, the KV cache footprint increases and becomes a major bottleneck for throughput and latency. For modern Transformers, this cache can occupy multiple gigabytes. NVIDIA researchers have introduced KVTC (KV [&amp;#8230;] The post NVIDIA Researchers Introduce KVTC Transform Coding Pipeline to Compress Key-Value Caches by 20x for Efficient LLM Serving appeared first on MarkTechPost .</description>
<source url="https://www.marktechpost.com/feed">marktechpost.com</source>
<category>ai</category>
<category>ai infrastructure</category>
<category>ai paper summary</category>
<category>ai shorts</category>
<category>applications</category>
<category>artificial intelligence</category>
<category>editors pick</category>
<category>llm</category>
<category>marktechpost.com</category>
<category>news</category>
<category>nonpaper</category>
<category>reasoning</category>
<category>serving</category>
<category>staff</category>
<category>tech news</category>
<category>technology</category>
</item>
<item>
<title>What Are LLM Parameters? A Simple Explanation of Weights, Biases, and Scale</title>
<link>https://pub.towardsai.net/what-are-llm-parameters-a-simple-explanation-of-weights-biases-and-scale-c2dde8945738?source=rss----98111c9905da---4</link>
<guid>title:what are llm parameters a simple explanation of weights biases and scale</guid>
<pubDate>Wed, 11 Feb 2026 03:20:09 +0000</pubDate>
<description>No complicated words. Just real talk about how this stuff works. Large Language Models (LLMs) like GPT, LLaMA, and Mistral contain billions of parameters, primarily weights and biases, that define how the model understands language. Everyone talks about ‚ÄúGPT-4 has 1. 76 trillion parameters‚Äù like it‚Äôs supposed to mean something. But most people just smile and pretend they understand.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>ai</category>
<category>artificial-intelligence</category>
<category>deep-learning</category>
<category>efficiency</category>
<category>llm</category>
<category>llm-parameters</category>
<category>machine-learning</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>rl</category>
</item>
<item>
<title>Samsung Galaxy Unpacked 2026: Everything we're expecting from the S26 launch on February 25</title>
<link>https://www.engadget.com/mobile/smartphones/samsung-galaxy-unpacked-2026-everything-were-expecting-from-the-s26-launch-on-february-25-130000524.html?src=rss</link>
<guid>title:samsung galaxy unpacked 2026 everything we re expecting from the s26 launch on february 25</guid>
<pubDate>Wed, 11 Feb 2026 00:17:12 +0000</pubDate>
<description>Samsung‚Äôs 2025 was filled with new foldables , an ultra-thin new form factor and the launch of Google's XR platform . After making some announcements at CES 2026 , the company has just announced its first Galaxy Unpacked of the year will take place on February 25, where it is expected to introduce the Galaxy S26 lineup. Official invites have just been shared, but actual information on what devices are arriving then is still unknown. But as usual, we know a lot about what‚Äôs expected at Unpacked. Engadget will be covering Galaxy Unpacked live, and we'll most likely have hands-on coverage of Samsung's new smartphones soon after they're announced. While we wait for the full details, here's everything we expect Samsung will introduce at the first Galaxy Unpacked event of 2026.</description>
<source url="https://www.engadget.com/rss.xml">engadget.com</source>
<category>ai</category>
<category>alignment</category>
<category>author_name|ian carlos campbell</category>
<category>engadget.com</category>
<category>handheld &amp; connected devices</category>
<category>headline</category>
<category>language|en-us</category>
<category>news</category>
<category>nonpaper</category>
<category>provider_name|engadget</category>
<category>region|us</category>
<category>rl</category>
<category>site|engadget</category>
<category>smart phones</category>
<category>technology &amp; electronics</category>
<category>vision</category>
</item>
<item>
<title>Anthropic‚Äôs Improved Workflow: When Your Hacks Ship as Features</title>
<link>https://pub.towardsai.net/anthropics-improved-workflow-when-your-hacks-ship-as-features-62e9cb79c856?source=rss----98111c9905da---4</link>
<guid>title:anthropic s improved workflow when your hacks ship as features</guid>
<pubDate>Tue, 10 Feb 2026 23:01:02 +0000</pubDate>
<description>Hiya! My name is Nick and I love writing Web apps! I have been writing in various languages both front-end and back-end professionally since ‚Äô98. My workflow, needless to say, has evolved considerably over that time. There are incredible workflow improvement announcements coming out of Anthropic. Initially I was excited about the 1m context window but after digging deeper there are more meaningful impacts from other recent improvements to Claude Code.</description>
<source url="https://pub.towardsai.net/feed">pub.towardsai.net</source>
<category>agents</category>
<category>ai</category>
<category>anthropic-claude</category>
<category>claude-code</category>
<category>news</category>
<category>nonpaper</category>
<category>pub.towardsai.net</category>
<category>reasoning</category>
<category>rl</category>
</item>
</channel>
</rss>